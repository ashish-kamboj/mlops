{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3355a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters (will be overridden by Papermill)\n",
    "model_type = 'linear_regression'\n",
    "output_dir = 'outputs'\n",
    "generate_plots = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2e81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "print(f\"Evaluating model: {model_type}\")\n",
    "print(f\"Generate plots: {generate_plots}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877824e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and metrics\n",
    "model_dir = os.path.join(output_dir, 'models')\n",
    "predictions_dir = os.path.join(output_dir, 'predictions')\n",
    "\n",
    "with open(os.path.join(model_dir, f'{model_type}_model.pkl'), 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(model_dir, f'{model_type}_metrics.json'), 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "# Load predictions\n",
    "train_pred = pd.read_csv(os.path.join(predictions_dir, f'{model_type}_train_predictions.csv'))\n",
    "test_pred = pd.read_csv(os.path.join(predictions_dir, f'{model_type}_test_predictions.csv'))\n",
    "\n",
    "print(f\"Model loaded successfully\")\n",
    "print(f\"Metrics loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f228a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed metrics\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"MODEL EVALUATION REPORT: {model_type.upper()}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  - Type: {metrics['model_type']}\")\n",
    "print(f\"  - Alpha: {metrics['alpha']}\")\n",
    "print(f\"  - Fit Intercept: {metrics['fit_intercept']}\")\n",
    "print(f\"  - Training Time: {metrics['training_time_seconds']:.4f}s\")\n",
    "\n",
    "print(f\"\\nTraining Set Metrics:\")\n",
    "for key, value in metrics['train_metrics'].items():\n",
    "    print(f\"  - {key.upper()}: {value:.6f}\")\n",
    "\n",
    "print(f\"\\nTest Set Metrics:\")\n",
    "for key, value in metrics['test_metrics'].items():\n",
    "    print(f\"  - {key.upper()}: {value:.6f}\")\n",
    "\n",
    "if metrics['coefficients']:\n",
    "    print(f\"\\nModel Coefficients (first 5):\")\n",
    "    for i, coef in enumerate(metrics['coefficients'][:5]):\n",
    "        print(f\"  - Feature {i}: {coef:.6f}\")\n",
    "    if len(metrics['coefficients']) > 5:\n",
    "        print(f\"  ... and {len(metrics['coefficients']) - 5} more\")\n",
    "\n",
    "if metrics['intercept']:\n",
    "    print(f\"\\nIntercept: {metrics['intercept']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd48401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "train_residuals = train_pred['actual'] - train_pred['predicted']\n",
    "test_residuals = test_pred['actual'] - test_pred['predicted']\n",
    "\n",
    "print(f\"\\nResidual Analysis:\")\n",
    "print(f\"\\nTraining Set Residuals:\")\n",
    "print(f\"  - Mean: {train_residuals.mean():.6f}\")\n",
    "print(f\"  - Std Dev: {train_residuals.std():.6f}\")\n",
    "print(f\"  - Min: {train_residuals.min():.6f}\")\n",
    "print(f\"  - Max: {train_residuals.max():.6f}\")\n",
    "\n",
    "print(f\"\\nTest Set Residuals:\")\n",
    "print(f\"  - Mean: {test_residuals.mean():.6f}\")\n",
    "print(f\"  - Std Dev: {test_residuals.std():.6f}\")\n",
    "print(f\"  - Min: {test_residuals.min():.6f}\")\n",
    "print(f\"  - Max: {test_residuals.max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b9933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plots if enabled\n",
    "if generate_plots:\n",
    "    print(f\"\\nGenerating visualization plots...\")\n",
    "    plots_dir = os.path.join(output_dir, 'plots')\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Set style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Plot 1: Actual vs Predicted (Train)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(train_pred['actual'], train_pred['predicted'], alpha=0.5, label='Predictions')\n",
    "    plt.plot([train_pred['actual'].min(), train_pred['actual'].max()],\n",
    "             [train_pred['actual'].min(), train_pred['actual'].max()],\n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f'{model_type} - Training Set: Actual vs Predicted')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, f'{model_type}_train_actual_vs_predicted.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 2: Actual vs Predicted (Test)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(test_pred['actual'], test_pred['predicted'], alpha=0.5, color='orange', label='Predictions')\n",
    "    plt.plot([test_pred['actual'].min(), test_pred['actual'].max()],\n",
    "             [test_pred['actual'].min(), test_pred['actual'].max()],\n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f'{model_type} - Test Set: Actual vs Predicted')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, f'{model_type}_test_actual_vs_predicted.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 3: Residuals Distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    axes[0].hist(train_residuals, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "    axes[0].set_xlabel('Residuals')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title(f'{model_type} - Training Set Residuals Distribution')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].hist(test_residuals, bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[1].set_xlabel('Residuals')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title(f'{model_type} - Test Set Residuals Distribution')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, f'{model_type}_residuals_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 4: Residuals vs Predicted\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    axes[0].scatter(train_pred['predicted'], train_residuals, alpha=0.5, color='blue')\n",
    "    axes[0].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    axes[0].set_xlabel('Predicted Values')\n",
    "    axes[0].set_ylabel('Residuals')\n",
    "    axes[0].set_title(f'{model_type} - Training Set: Residuals vs Predicted')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].scatter(test_pred['predicted'], test_residuals, alpha=0.5, color='orange')\n",
    "    axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    axes[1].set_xlabel('Predicted Values')\n",
    "    axes[1].set_ylabel('Residuals')\n",
    "    axes[1].set_title(f'{model_type} - Test Set: Residuals vs Predicted')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, f'{model_type}_residuals_vs_predicted.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\nPlots saved to {plots_dir}\")\n",
    "    print(f\"Files created:\")\n",
    "    print(f\"  - {model_type}_train_actual_vs_predicted.png\")\n",
    "    print(f\"  - {model_type}_test_actual_vs_predicted.png\")\n",
    "    print(f\"  - {model_type}_residuals_distribution.png\")\n",
    "    print(f\"  - {model_type}_residuals_vs_predicted.png\")\n",
    "else:\n",
    "    print(f\"\\nPlot generation disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0787f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation summary\n",
    "evaluation_summary = {\n",
    "    'model_type': model_type,\n",
    "    'evaluation_timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'metrics': metrics,\n",
    "    'residual_analysis': {\n",
    "        'train': {\n",
    "            'mean': float(train_residuals.mean()),\n",
    "            'std': float(train_residuals.std()),\n",
    "            'min': float(train_residuals.min()),\n",
    "            'max': float(train_residuals.max())\n",
    "        },\n",
    "        'test': {\n",
    "            'mean': float(test_residuals.mean()),\n",
    "            'std': float(test_residuals.std()),\n",
    "            'min': float(test_residuals.min()),\n",
    "            'max': float(test_residuals.max())\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "eval_path = os.path.join(output_dir, 'models', f'{model_type}_evaluation_summary.json')\n",
    "with open(eval_path, 'w') as f:\n",
    "    json.dump(evaluation_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nEvaluation summary saved to {eval_path}\")\n",
    "print(f\"\\nEvaluation completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
