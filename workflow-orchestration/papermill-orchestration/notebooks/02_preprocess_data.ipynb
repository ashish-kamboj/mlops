{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ec455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters (will be overridden by Papermill)\n",
    "normalize = True\n",
    "data_dir = 'outputs/data'\n",
    "output_dir = 'outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d3f253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(f\"Preprocessing data with parameters:\")\n",
    "print(f\"  - normalize: {normalize}\")\n",
    "print(f\"  - data_dir: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ccbfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from previous step\n",
    "X_train = pd.read_csv(os.path.join(data_dir, 'X_train.csv'))\n",
    "X_test = pd.read_csv(os.path.join(data_dir, 'X_test.csv'))\n",
    "y_train = pd.read_csv(os.path.join(data_dir, 'y_train.csv')).values.ravel()\n",
    "y_test = pd.read_csv(os.path.join(data_dir, 'y_test.csv')).values.ravel()\n",
    "\n",
    "print(f\"Data loaded successfully\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d85a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(f\"\\nMissing values in X_train: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in X_test: {X_test.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in y_train: {pd.Series(y_train).isnull().sum()}\")\n",
    "print(f\"Missing values in y_test: {pd.Series(y_test).isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e6de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features if enabled\n",
    "if normalize:\n",
    "    print(f\"\\nApplying StandardScaler normalization...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"X_train mean after scaling: {X_train_scaled.mean(axis=0)[:3]}...\") # Show first 3 features\n",
    "    print(f\"X_train std after scaling: {X_train_scaled.std(axis=0)[:3]}...\")\n",
    "else:\n",
    "    print(f\"\\nSkipping normalization\")\n",
    "    X_train_scaled = X_train.values\n",
    "    X_test_scaled = X_test.values\n",
    "    scaler = None\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train_processed = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_processed = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59543300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data\n",
    "processed_dir = os.path.join(output_dir, 'processed')\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "X_train_processed.to_csv(os.path.join(processed_dir, 'X_train_processed.csv'), index=False)\n",
    "X_test_processed.to_csv(os.path.join(processed_dir, 'X_test_processed.csv'), index=False)\n",
    "y_train_df = pd.DataFrame({'target': y_train})\n",
    "y_test_df = pd.DataFrame({'target': y_test})\n",
    "y_train_df.to_csv(os.path.join(processed_dir, 'y_train.csv'), index=False)\n",
    "y_test_df.to_csv(os.path.join(processed_dir, 'y_test.csv'), index=False)\n",
    "\n",
    "# Save scaler if used\n",
    "if scaler is not None:\n",
    "    with open(os.path.join(processed_dir, 'scaler.pkl'), 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "# Save preprocessing metadata\n",
    "preprocess_metadata = {\n",
    "    'normalize': normalize,\n",
    "    'scaler_used': scaler is not None,\n",
    "    'scaler_type': 'StandardScaler' if scaler else None\n",
    "}\n",
    "\n",
    "with open(os.path.join(processed_dir, 'preprocessing_metadata.json'), 'w') as f:\n",
    "    json.dump(preprocess_metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nPreprocessed data saved to {processed_dir}\")\n",
    "print(f\"Files created:\")\n",
    "print(f\"  - X_train_processed.csv\")\n",
    "print(f\"  - X_test_processed.csv\")\n",
    "print(f\"  - y_train.csv\")\n",
    "print(f\"  - y_test.csv\")\n",
    "if scaler is not None:\n",
    "    print(f\"  - scaler.pkl\")\n",
    "print(f\"  - preprocessing_metadata.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
