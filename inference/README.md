## ML model Inferencing

|Tasks                                                                                                                                            |Details          |
|:------------------------------------------------------------------------------------------------------------------------------------------------|-----------------|
|[Inference and Quantization with ONNX](https://github.com/ashish-kamboj/mlops/blob/main/inference/inference_using_onnx_and%20quantization.ipynb)|1. Convert sklearn model to ONNX format <br> 2. Inferencing with ONNX Runtime <br> 3. Applying **Quantization** on ONNX model for reducing the size and make inferencing faster|

