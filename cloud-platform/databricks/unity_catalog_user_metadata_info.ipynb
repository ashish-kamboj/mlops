{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dec6534-1180-44fb-824d-6020a3ca6cbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Unity Catalog User Metadata Demo\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates the usage and importance of **User Metadata** in Unity Catalog Delta tables. User metadata is a powerful feature for tracking data lineage, audit trails, and operation history.\n",
    "\n",
    "## What is User Metadata?\n",
    "* **User Metadata** is commit-level metadata stored in Delta Lake table history\n",
    "* It's **NOT a tag** - it's metadata attached to each table version/commit\n",
    "* Persists in the `DESCRIBE HISTORY` output for audit and lineage tracking\n",
    "* Can store any custom JSON string with operation context\n",
    "\n",
    "## Use Cases\n",
    "* **Data Lineage**: Track which ETL job or process modified the data\n",
    "* **Audit Trails**: Record operation type, user, timestamp, and run IDs\n",
    "* **Compliance**: Maintain detailed history of data modifications\n",
    "* **Debugging**: Identify which operation caused data issues\n",
    "\n",
    "## What We'll Demonstrate\n",
    "We'll create a customer table and perform INSERT, UPDATE, and DELETE operations, each tagged with:\n",
    "* `runid`: Unique UUID for tracking the operation\n",
    "* `state`: Operation type (Insert/Update/Delete)\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de5c2bbc-dfc2-4946-af73-ab93dd84bd1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Setup and Configuration\n",
    "\n",
    "In this section, we'll:\n",
    "1. Import required libraries for Delta Lake operations\n",
    "2. Define the target Unity Catalog table\n",
    "3. Create sample customer data for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aef6226c-6dac-486a-9dac-20a05c7c2a04",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import libraries and setup"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target table: datafabric_catalog.ml_outputs.customer_info\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import uuid  # For generating unique run IDs\n",
    "import json  # For creating JSON-formatted user metadata\n",
    "from pyspark.sql import SparkSession  # Spark session (already available)\n",
    "from pyspark.sql.functions import col, lit  # Column operations\n",
    "from delta.tables import DeltaTable  # Delta Lake table operations\n",
    "\n",
    "# Define the target Unity Catalog table\n",
    "# Format: catalog.schema.table\n",
    "table_name = \"datafabric_catalog.ml_outputs.customer_info\"\n",
    "\n",
    "print(f\"Target table: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea5acf27-18b7-44cf-beca-d63f65b38b1c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create sample customer data"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial customer data created:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>name</th><th>email</th><th>city</th><th>status</th></tr></thead><tbody><tr><td>1</td><td>John Doe</td><td>john.doe@email.com</td><td>New York</td><td>Active</td></tr><tr><td>2</td><td>Jane Smith</td><td>jane.smith@email.com</td><td>Los Angeles</td><td>Active</td></tr><tr><td>3</td><td>Bob Johnson</td><td>bob.johnson@email.com</td><td>Chicago</td><td>Active</td></tr><tr><td>4</td><td>Alice Williams</td><td>alice.williams@email.com</td><td>Houston</td><td>Active</td></tr><tr><td>5</td><td>Charlie Brown</td><td>charlie.brown@email.com</td><td>Phoenix</td><td>Active</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "John Doe",
         "john.doe@email.com",
         "New York",
         "Active"
        ],
        [
         2,
         "Jane Smith",
         "jane.smith@email.com",
         "Los Angeles",
         "Active"
        ],
        [
         3,
         "Bob Johnson",
         "bob.johnson@email.com",
         "Chicago",
         "Active"
        ],
        [
         4,
         "Alice Williams",
         "alice.williams@email.com",
         "Houston",
         "Active"
        ],
        [
         5,
         "Charlie Brown",
         "charlie.brown@email.com",
         "Phoenix",
         "Active"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create sample customer data with proper schema\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Define the schema for customer table\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),  # Primary key, not nullable\n",
    "    StructField(\"name\", StringType(), True),           # Customer name\n",
    "    StructField(\"email\", StringType(), True),          # Email address\n",
    "    StructField(\"city\", StringType(), True),           # City location\n",
    "    StructField(\"status\", StringType(), True)          # Customer status (Active/Premium)\n",
    "])\n",
    "\n",
    "# Create initial customer data (5 sample records)\n",
    "initial_data = [\n",
    "    (1, \"John Doe\", \"john.doe@email.com\", \"New York\", \"Active\"),\n",
    "    (2, \"Jane Smith\", \"jane.smith@email.com\", \"Los Angeles\", \"Active\"),\n",
    "    (3, \"Bob Johnson\", \"bob.johnson@email.com\", \"Chicago\", \"Active\"),\n",
    "    (4, \"Alice Williams\", \"alice.williams@email.com\", \"Houston\", \"Active\"),\n",
    "    (5, \"Charlie Brown\", \"charlie.brown@email.com\", \"Phoenix\", \"Active\")\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_initial = spark.createDataFrame(initial_data, schema)\n",
    "print(\"Initial customer data created:\")\n",
    "display(df_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d90cc00-7b58-46b4-b732-060670bef0de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2: DML Operations with User Metadata\n",
    "\n",
    "Now we'll perform three types of operations, each with custom user metadata:\n",
    "\n",
    "### 1. INSERT Operation\n",
    "* Create the table and insert initial data\n",
    "* Attach metadata: `runid` (UUID) and `state=\"Insert\"`\n",
    "\n",
    "### 2. UPDATE Operation\n",
    "* Update customer status to \"Premium\" for specific cities\n",
    "* Attach metadata: `runid` (UUID) and `state=\"Update\"`\n",
    "\n",
    "### 3. DELETE Operation\n",
    "* Remove a customer record\n",
    "* Attach metadata: `runid` (UUID) and `state=\"Delete\"`\n",
    "\n",
    "Each operation will be tracked in the table history with its unique metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "028d5f1c-950d-4ad9-9d09-bae80f9388ed",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "INSERT: Create table with user metadata"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT Operation - RunID: cc4853ea-919b-483b-b2db-4fdb5076652c, State: Insert\n",
      "User Metadata: {\"runid\": \"cc4853ea-919b-483b-b2db-4fdb5076652c\", \"state\": \"Insert\"}\n",
      "\n",
      "‚úì Table created and initial data inserted with user metadata\n",
      "Table: datafabric_catalog.ml_outputs.customer_info\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# INSERT OPERATION WITH USER METADATA\n",
    "# ============================================\n",
    "\n",
    "# Step 1: Generate unique run ID for this operation\n",
    "run_id_insert = str(uuid.uuid4())  # UUID ensures uniqueness across all operations\n",
    "operation_state = \"Insert\"\n",
    "\n",
    "# Step 2: Create user metadata as JSON string\n",
    "# This metadata will be stored in Delta Lake commit history\n",
    "user_metadata = json.dumps({\n",
    "    \"runid\": run_id_insert,      # Unique identifier for this operation\n",
    "    \"state\": operation_state      # Type of operation performed\n",
    "})\n",
    "\n",
    "print(f\"INSERT Operation - RunID: {run_id_insert}, State: {operation_state}\")\n",
    "print(f\"User Metadata: {user_metadata}\")\n",
    "\n",
    "# Step 3: Write data to Unity Catalog table with user metadata\n",
    "# The .option(\"userMetadata\", ...) attaches metadata to this commit\n",
    "df_initial.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"userMetadata\", user_metadata) \\\n",
    "    .saveAsTable(table_name)\n",
    "\n",
    "print(f\"\\n‚úì Table created and initial data inserted with user metadata\")\n",
    "print(f\"Table: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d7dc2e-25c8-4269-ab8f-fd08350d52cf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "UPDATE: Update records with user metadata"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE Operation - RunID: 73963ced-fde5-42f9-8da3-dcb0ef448d87, State: Update\n",
      "User Metadata: {\"runid\": \"73963ced-fde5-42f9-8da3-dcb0ef448d87\", \"state\": \"Update\"}\n",
      "\n",
      "‚úì Records updated with user metadata\n",
      "Updated customers in New York and Chicago to Premium status\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# UPDATE OPERATION WITH USER METADATA\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Step 1: Generate unique run ID for this UPDATE operation\n",
    "run_id_update = str(uuid.uuid4())\n",
    "operation_state = \"Update\"\n",
    "\n",
    "# Step 2: Create user metadata for this update\n",
    "user_metadata = json.dumps({\n",
    "    \"runid\": run_id_update,\n",
    "    \"state\": operation_state\n",
    "})\n",
    "\n",
    "print(f\"UPDATE Operation - RunID: {run_id_update}, State: {operation_state}\")\n",
    "print(f\"User Metadata: {user_metadata}\")\n",
    "\n",
    "# Step 3: Read current table data\n",
    "df_current = spark.table(table_name)\n",
    "\n",
    "# Step 4: Transform data - Update status to \"Premium\" for customers in NY and Chicago\n",
    "df_updated = df_current.withColumn(\n",
    "    \"status\",\n",
    "    when(col(\"city\").isin([\"New York\", \"Chicago\"]), lit(\"Premium\"))\n",
    "    .otherwise(col(\"status\"))  # Keep existing status for other cities\n",
    ")\n",
    "\n",
    "# Step 5: Write back the updated data with user metadata\n",
    "# Using overwrite mode to replace the entire table (serverless-compatible approach)\n",
    "df_updated.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"userMetadata\", user_metadata) \\\n",
    "    .option(\"overwriteSchema\", \"false\") \\\n",
    "    .saveAsTable(table_name)\n",
    "\n",
    "print(f\"\\n‚úì Records updated with user metadata\")\n",
    "print(f\"Updated customers in New York and Chicago to Premium status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4be9cbbe-cbeb-4b0a-8b46-ffdeddb9b125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚ö†Ô∏è Important Note: Serverless Compute Compatibility\n",
    "\n",
    "On **serverless compute**, the Spark configuration `spark.databricks.delta.commitInfo.userMetadata` is **not supported**.\n",
    "\n",
    "#### ‚úÖ Recommended Solution (Works on All Compute Types)\n",
    "Use the `.option(\"userMetadata\", ...)` approach with DataFrame write operations:\n",
    "\n",
    "* **INSERT**: `df.write.option(\"userMetadata\", ...).saveAsTable()`\n",
    "* **UPDATE**: Read table ‚Üí Transform with `withColumn()` ‚Üí Write with `option(\"userMetadata\", ...)`\n",
    "* **DELETE**: Read table ‚Üí Filter data ‚Üí Write with `option(\"userMetadata\", ...)`\n",
    "\n",
    "This approach works on both **serverless** and **classic compute**, making it the most portable solution.\n",
    "\n",
    "#### ‚õî Alternative for Classic Compute Only\n",
    "```python\n",
    "# This works ONLY on classic compute (not serverless)\n",
    "spark.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", metadata)\n",
    "delta_table.update(...)  # or .delete() or .merge()\n",
    "spark.conf.unset(\"spark.databricks.delta.commitInfo.userMetadata\")\n",
    "```\n",
    "\n",
    "**Why the difference?**  \n",
    "Serverless compute restricts most Spark configurations for security and performance optimization. Only a limited set of configurations are allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05788e2c-9b4d-458d-a962-15fb853ee5ce",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DELETE: Delete records with user metadata"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELETE Operation - RunID: eea06d6a-2c38-4b66-8d21-948857daac81, State: Delete\n",
      "User Metadata: {\"runid\": \"eea06d6a-2c38-4b66-8d21-948857daac81\", \"state\": \"Delete\"}\n",
      "\n",
      "‚úì Record deleted with user metadata\n",
      "Deleted customer with ID 5\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DELETE OPERATION WITH USER METADATA\n",
    "# ============================================\n",
    "\n",
    "# Step 1: Generate unique run ID for this DELETE operation\n",
    "run_id_delete = str(uuid.uuid4())\n",
    "operation_state = \"Delete\"\n",
    "\n",
    "# Step 2: Create user metadata for this delete\n",
    "user_metadata = json.dumps({\n",
    "    \"runid\": run_id_delete,\n",
    "    \"state\": operation_state\n",
    "})\n",
    "\n",
    "print(f\"DELETE Operation - RunID: {run_id_delete}, State: {operation_state}\")\n",
    "print(f\"User Metadata: {user_metadata}\")\n",
    "\n",
    "# Step 3: Read current table data\n",
    "df_current = spark.table(table_name)\n",
    "\n",
    "# Step 4: Filter out the record to delete (customer_id = 5)\n",
    "# This simulates a DELETE operation by excluding the target record\n",
    "df_filtered = df_current.filter(col(\"customer_id\") != 5)\n",
    "\n",
    "# Step 5: Write back the filtered data with user metadata\n",
    "# The result is equivalent to DELETE WHERE customer_id = 5\n",
    "df_filtered.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"userMetadata\", user_metadata) \\\n",
    "    .option(\"overwriteSchema\", \"false\") \\\n",
    "    .saveAsTable(table_name)\n",
    "\n",
    "print(f\"\\n‚úì Record deleted with user metadata\")\n",
    "print(f\"Deleted customer with ID 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0bb6e32-3919-4870-96a3-57f2525d898c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: Verify User Metadata in Table History\n",
    "\n",
    "Now let's verify that our user metadata was successfully stored in the Delta Lake table history.\n",
    "\n",
    "We'll:\n",
    "1. View the current table data after all operations\n",
    "2. Query the table history using `DESCRIBE HISTORY`\n",
    "3. Parse and display the user metadata in a readable format\n",
    "\n",
    "The `userMetadata` column in the history will show our custom tracking information for each operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bdea42c-f9af-4e5b-852f-1e744db12847",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View current table data"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current table data after all operations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>name</th><th>email</th><th>city</th><th>status</th></tr></thead><tbody><tr><td>1</td><td>John Doe</td><td>john.doe@email.com</td><td>New York</td><td>Premium</td></tr><tr><td>2</td><td>Jane Smith</td><td>jane.smith@email.com</td><td>Los Angeles</td><td>Active</td></tr><tr><td>3</td><td>Bob Johnson</td><td>bob.johnson@email.com</td><td>Chicago</td><td>Premium</td></tr><tr><td>4</td><td>Alice Williams</td><td>alice.williams@email.com</td><td>Houston</td><td>Active</td></tr><tr><td>5</td><td>Charlie Brown</td><td>charlie.brown@email.com</td><td>Phoenix</td><td>Active</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "John Doe",
         "john.doe@email.com",
         "New York",
         "Premium"
        ],
        [
         2,
         "Jane Smith",
         "jane.smith@email.com",
         "Los Angeles",
         "Active"
        ],
        [
         3,
         "Bob Johnson",
         "bob.johnson@email.com",
         "Chicago",
         "Premium"
        ],
        [
         4,
         "Alice Williams",
         "alice.williams@email.com",
         "Houston",
         "Active"
        ],
        [
         5,
         "Charlie Brown",
         "charlie.brown@email.com",
         "Phoenix",
         "Active"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================\n",
    "# VIEW CURRENT TABLE DATA\n",
    "# ============================================\n",
    "\n",
    "# Display the current state of the table after all operations\n",
    "# This shows the final result of INSERT, UPDATE, and DELETE operations\n",
    "print(\"Current table data after all operations:\")\n",
    "df_current = spark.table(table_name)\n",
    "display(df_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2074a36a-6bf0-4fd8-94f5-4f4aed274796",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View table history with user metadata"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table History with User Metadata:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>operation</th><th>operationParameters</th><th>userMetadata</th></tr></thead><tbody><tr><td>1</td><td>2025-12-10T15:59:00.000Z</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>Map(partitionBy -> [], clusterBy -> [], description -> null, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> true)</td><td>{\"runid\": \"73963ced-fde5-42f9-8da3-dcb0ef448d87\", \"state\": \"Update\"}</td></tr><tr><td>0</td><td>2025-12-10T15:51:43.000Z</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>Map(partitionBy -> [], clusterBy -> [], description -> null, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> true)</td><td>{\"runid\": \"cc4853ea-919b-483b-b2db-4fdb5076652c\", \"state\": \"Insert\"}</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "2025-12-10T15:59:00.000Z",
         "CREATE OR REPLACE TABLE AS SELECT",
         {
          "clusterBy": "[]",
          "description": null,
          "isManaged": "true",
          "partitionBy": "[]",
          "properties": "{\"delta.enableDeletionVectors\":\"true\"}",
          "statsOnLoad": "true"
         },
         "{\"runid\": \"73963ced-fde5-42f9-8da3-dcb0ef448d87\", \"state\": \"Update\"}"
        ],
        [
         0,
         "2025-12-10T15:51:43.000Z",
         "CREATE OR REPLACE TABLE AS SELECT",
         {
          "clusterBy": "[]",
          "description": null,
          "isManaged": "true",
          "partitionBy": "[]",
          "properties": "{\"delta.enableDeletionVectors\":\"true\"}",
          "statsOnLoad": "true"
         },
         "{\"runid\": \"cc4853ea-919b-483b-b2db-4fdb5076652c\", \"state\": \"Insert\"}"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "User Metadata Explanation:\n",
      "The 'userMetadata' column shows the custom metadata attached to each operation.\n",
      "Each operation has a unique 'runid' and 'state' (Insert/Update/Delete).\n",
      "This demonstrates how user metadata can track operation lineage and audit trails.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# VIEW TABLE HISTORY WITH USER METADATA\n",
    "# ============================================\n",
    "\n",
    "print(\"Table History with User Metadata:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 1: Get Delta table reference\n",
    "delta_table = DeltaTable.forName(spark, table_name)\n",
    "\n",
    "# Step 2: Retrieve full table history\n",
    "# This includes all commits/versions with their metadata\n",
    "history_df = delta_table.history()\n",
    "\n",
    "# Step 3: Select relevant columns for display\n",
    "# Focus on version, timestamp, operation type, and our custom userMetadata\n",
    "history_display = history_df.select(\n",
    "    \"version\",              # Table version number (increments with each operation)\n",
    "    \"timestamp\",            # When the operation occurred\n",
    "    \"operation\",            # Type of operation (CREATE, UPDATE, DELETE, etc.)\n",
    "    \"operationParameters\",  # Parameters passed to the operation\n",
    "    \"userMetadata\"          # Our custom metadata (runid and state)\n",
    ").orderBy(col(\"version\").desc())  # Show most recent operations first\n",
    "\n",
    "# Display the history\n",
    "display(history_display)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"User Metadata Explanation:\")\n",
    "print(\"The 'userMetadata' column shows the custom metadata attached to each operation.\")\n",
    "print(\"Each operation has a unique 'runid' and 'state' (Insert/Update/Delete).\")\n",
    "print(\"This demonstrates how user metadata can track operation lineage and audit trails.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67fc795a-1e36-4956-94cf-b4bcc0b4dc15",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parse and display user metadata details"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed User Metadata Details:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>operation</th><th>run_id</th><th>operation_state</th></tr></thead><tbody><tr><td>1</td><td>2025-12-10T15:59:00.000Z</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>73963ced-fde5-42f9-8da3-dcb0ef448d87</td><td>Update</td></tr><tr><td>0</td><td>2025-12-10T15:51:43.000Z</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>cc4853ea-919b-483b-b2db-4fdb5076652c</td><td>Insert</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "2025-12-10T15:59:00.000Z",
         "CREATE OR REPLACE TABLE AS SELECT",
         "73963ced-fde5-42f9-8da3-dcb0ef448d87",
         "Update"
        ],
        [
         0,
         "2025-12-10T15:51:43.000Z",
         "CREATE OR REPLACE TABLE AS SELECT",
         "cc4853ea-919b-483b-b2db-4fdb5076652c",
         "Insert"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "run_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation_state",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Key Insights:\n",
      "* Each operation has a unique UUID as run_id for tracking\n",
      "* The operation_state clearly identifies Insert/Update/Delete operations\n",
      "* This metadata persists in table history and can be queried anytime\n",
      "* User metadata is NOT a tag - it's commit-level metadata in Delta Lake history\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# PARSE AND DISPLAY USER METADATA DETAILS\n",
    "# ============================================\n",
    "\n",
    "# Parse the JSON user metadata for better readability\n",
    "from pyspark.sql.functions import from_json, schema_of_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "print(\"Parsed User Metadata Details:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 1: Define schema for the user metadata JSON\n",
    "# This matches the structure we created: {\"runid\": \"...\", \"state\": \"...\"}\n",
    "metadata_schema = StructType([\n",
    "    StructField(\"runid\", StringType(), True),   # UUID for the operation\n",
    "    StructField(\"state\", StringType(), True)    # Operation type (Insert/Update/Delete)\n",
    "])\n",
    "\n",
    "# Step 2: Parse the userMetadata JSON column and extract fields\n",
    "history_parsed = history_df.select(\n",
    "    \"version\",\n",
    "    \"timestamp\",\n",
    "    \"operation\",\n",
    "    from_json(col(\"userMetadata\"), metadata_schema).alias(\"metadata\")  # Parse JSON\n",
    ").select(\n",
    "    \"version\",\n",
    "    \"timestamp\",\n",
    "    \"operation\",\n",
    "    col(\"metadata.runid\").alias(\"run_id\"),              # Extract runid\n",
    "    col(\"metadata.state\").alias(\"operation_state\")      # Extract state\n",
    ").orderBy(col(\"version\").desc())\n",
    "\n",
    "# Display parsed metadata in a clean table format\n",
    "display(history_parsed)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Key Insights:\")\n",
    "print(\"* Each operation has a unique UUID as run_id for tracking\")\n",
    "print(\"* The operation_state clearly identifies Insert/Update/Delete operations\")\n",
    "print(\"* This metadata persists in table history and can be queried anytime\")\n",
    "print(\"* User metadata is NOT a tag - it's commit-level metadata in Delta Lake history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5532e805-dc76-4ad7-a0b4-772efebc8dd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Accomplished\n",
    "‚úì Created a Unity Catalog table with sample customer data  \n",
    "‚úì Performed INSERT, UPDATE, and DELETE operations  \n",
    "‚úì Attached custom user metadata to each operation  \n",
    "‚úì Verified metadata persistence in table history  \n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "#### 1. User Metadata vs Tags\n",
    "* **User Metadata**: Commit-level metadata stored in Delta Lake history\n",
    "* **Tags**: Table-level or column-level labels (different feature)\n",
    "* User metadata is **version-specific** and tracks individual operations\n",
    "\n",
    "#### 2. How to Set User Metadata\n",
    "\n",
    "**Method 1: DataFrame Write (Serverless Compatible)** ‚úÖ\n",
    "```python\n",
    "df.write \\\n",
    "    .option(\"userMetadata\", json_string) \\\n",
    "    .saveAsTable(table_name)\n",
    "```\n",
    "\n",
    "**Method 2: Spark Configuration (Classic Compute Only)** ‚ö†Ô∏è\n",
    "```python\n",
    "spark.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", json_string)\n",
    "delta_table.update(...)\n",
    "spark.conf.unset(\"spark.databricks.delta.commitInfo.userMetadata\")\n",
    "```\n",
    "\n",
    "#### 3. Use Cases\n",
    "* **ETL Pipelines**: Track job IDs, run timestamps, and data sources\n",
    "* **Compliance**: Maintain audit trails for regulatory requirements\n",
    "* **Data Quality**: Record validation results and data quality scores\n",
    "* **Debugging**: Identify which process caused data issues\n",
    "* **Lineage**: Build end-to-end data lineage tracking\n",
    "\n",
    "#### 4. Querying User Metadata\n",
    "```python\n",
    "# Get table history\n",
    "history_df = DeltaTable.forName(spark, table_name).history()\n",
    "\n",
    "# Filter by metadata\n",
    "history_df.filter(col(\"userMetadata\").contains(\"Insert\"))\n",
    "```\n",
    "\n",
    "### Best Practices\n",
    "1. **Use JSON format** for structured metadata\n",
    "2. **Include unique identifiers** (UUIDs, job IDs) for tracking\n",
    "3. **Keep metadata concise** - avoid large payloads\n",
    "4. **Document your schema** - maintain consistency across operations\n",
    "5. **Use serverless-compatible approach** for portability\n",
    "\n",
    "### Next Steps\n",
    "* Integrate user metadata into your ETL pipelines\n",
    "* Build automated lineage tracking systems\n",
    "* Create audit reports using table history\n",
    "* Implement data quality tracking with metadata"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "unity_catalog_metadata_info",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
