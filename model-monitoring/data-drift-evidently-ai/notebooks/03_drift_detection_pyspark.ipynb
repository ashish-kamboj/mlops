{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dba3a71-f33a-438a-b5cf-6ed1599ee6f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evidently==0.7.19 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (0.7.19)\nRequirement already satisfied: pyyaml==6.0.2 in /databricks/python3/lib/python3.12/site-packages (6.0.2)\nRequirement already satisfied: pandas==2.2.3 in /databricks/python3/lib/python3.12/site-packages (2.2.3)\nRequirement already satisfied: numpy==2.2.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (2.2.1)\nRequirement already satisfied: azure-storage-file-datalake==12.20.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (12.20.0)\nRequirement already satisfied: azure-identity==1.19.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (1.19.0)\nRequirement already satisfied: plotly==5.24.1 in /databricks/python3/lib/python3.12/site-packages (5.24.1)\nRequirement already satisfied: certifi>=2024.7.4 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (2025.1.31)\nRequirement already satisfied: cryptography>=43.0.1 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (43.0.3)\nRequirement already satisfied: deprecation>=2.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (2.1.0)\nRequirement already satisfied: dynaconf>=3.2.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (3.2.12)\nRequirement already satisfied: fsspec>=2024.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (2026.1.0)\nRequirement already satisfied: iterative-telemetry>=0.0.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (0.0.10)\nRequirement already satisfied: litestar>=2.19.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (2.19.0)\nRequirement already satisfied: nltk>=3.6.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (3.9.2)\nRequirement already satisfied: opentelemetry-proto>=1.25.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (1.39.1)\nRequirement already satisfied: pydantic>=1.10.16 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (2.10.6)\nRequirement already satisfied: requests>=2.32.0 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (2.32.3)\nRequirement already satisfied: rich>=13 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (13.9.4)\nRequirement already satisfied: scikit-learn>=1.1.1 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (1.6.1)\nRequirement already satisfied: scipy>=1.10.0 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (1.15.1)\nRequirement already satisfied: statsmodels>=0.12.2 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (0.14.4)\nRequirement already satisfied: typer>=0.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (0.21.1)\nRequirement already satisfied: typing-inspect>=0.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (0.9.0)\nRequirement already satisfied: ujson>=5.4.0 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (5.10.0)\nRequirement already satisfied: urllib3>=1.26.19 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (2.3.0)\nRequirement already satisfied: uuid6>=2024.7.10 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (2025.0.1)\nRequirement already satisfied: uvicorn>=0.22.0 in /databricks/python3/lib/python3.12/site-packages (from uvicorn[standard]>=0.22.0->evidently==0.7.19) (0.34.2)\nRequirement already satisfied: watchdog>=3.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (6.0.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /databricks/python3/lib/python3.12/site-packages (from pandas==2.2.3) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas==2.2.3) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas==2.2.3) (2024.1)\nRequirement already satisfied: azure-core>=1.30.0 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake==12.20.0) (1.34.0)\nRequirement already satisfied: azure-storage-blob>=12.25.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from azure-storage-file-datalake==12.20.0) (12.28.0)\nRequirement already satisfied: typing-extensions>=4.6.0 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake==12.20.0) (4.12.2)\nRequirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake==12.20.0) (0.6.1)\nRequirement already satisfied: msal>=1.30.0 in /databricks/python3/lib/python3.12/site-packages (from azure-identity==1.19.0) (1.32.3)\nRequirement already satisfied: msal-extensions>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from azure-identity==1.19.0) (1.3.1)\nRequirement already satisfied: tenacity>=6.2.0 in /databricks/python3/lib/python3.12/site-packages (from plotly==5.24.1) (9.0.0)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.12/site-packages (from plotly==5.24.1) (24.1)\nRequirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core>=1.30.0->azure-storage-file-datalake==12.20.0) (1.16.0)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.12/site-packages (from cryptography>=43.0.1->evidently==0.7.19) (1.17.1)\nRequirement already satisfied: appdirs in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from iterative-telemetry>=0.0.5->evidently==0.7.19) (1.4.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from iterative-telemetry>=0.0.5->evidently==0.7.19) (3.18.0)\nRequirement already satisfied: distro in /usr/lib/python3/dist-packages (from iterative-telemetry>=0.0.5->evidently==0.7.19) (1.9.0)\nRequirement already satisfied: anyio>=3 in /databricks/python3/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (4.6.2)\nRequirement already satisfied: click in /databricks/python3/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (8.1.7)\nRequirement already satisfied: httpx>=0.22 in /databricks/python3/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (0.27.0)\nRequirement already satisfied: litestar-htmx>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (0.5.0)\nRequirement already satisfied: msgspec>=0.18.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (0.20.0)\nRequirement already satisfied: multidict>=6.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (6.7.0)\nRequirement already satisfied: multipart>=1.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (1.3.0)\nRequirement already satisfied: polyfactory>=2.6.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (3.2.0)\nRequirement already satisfied: rich-click in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (1.9.5)\nRequirement already satisfied: sniffio>=1.3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (1.3.1)\nRequirement already satisfied: PyJWT<3,>=1.0.0 in /databricks/python3/lib/python3.12/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity==1.19.0) (2.10.1)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.12/site-packages (from nltk>=3.6.7->evidently==0.7.19) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from nltk>=3.6.7->evidently==0.7.19) (2026.1.15)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from nltk>=3.6.7->evidently==0.7.19) (4.67.1)\nRequirement already satisfied: protobuf<7.0,>=5.0 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-proto>=1.25.0->evidently==0.7.19) (5.29.4)\nRequirement already satisfied: pyarrow>=10.0.1 in /databricks/python3/lib/python3.12/site-packages (from pandas[parquet]>=1.3.5->evidently==0.7.19) (19.0.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic>=1.10.16->evidently==0.7.19) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.2 in /databricks/python3/lib/python3.12/site-packages (from pydantic>=1.10.16->evidently==0.7.19) (2.27.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.32.0->evidently==0.7.19) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.32.0->evidently==0.7.19) (3.7)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /databricks/python3/lib/python3.12/site-packages (from rich>=13->evidently==0.7.19) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /databricks/python3/lib/python3.12/site-packages (from rich>=13->evidently==0.7.19) (2.15.1)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn>=1.1.1->evidently==0.7.19) (3.5.0)\nRequirement already satisfied: patsy>=0.5.6 in /databricks/python3/lib/python3.12/site-packages (from statsmodels>=0.12.2->evidently==0.7.19) (1.0.1)\nRequirement already satisfied: shellingham>=1.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from typer>=0.3->evidently==0.7.19) (1.5.4)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from typing-inspect>=0.9.0->evidently==0.7.19) (1.0.0)\nRequirement already satisfied: h11>=0.8 in /databricks/python3/lib/python3.12/site-packages (from uvicorn>=0.22.0->uvicorn[standard]>=0.22.0->evidently==0.7.19) (0.14.0)\nRequirement already satisfied: httptools>=0.6.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from uvicorn[standard]>=0.22.0->evidently==0.7.19) (0.7.1)\nRequirement already satisfied: python-dotenv>=0.13 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from uvicorn[standard]>=0.22.0->evidently==0.7.19) (1.2.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from uvicorn[standard]>=0.22.0->evidently==0.7.19) (0.22.1)\nRequirement already satisfied: watchfiles>=0.13 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from uvicorn[standard]>=0.22.0->evidently==0.7.19) (1.1.1)\nRequirement already satisfied: websockets>=10.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from uvicorn[standard]>=0.22.0->evidently==0.7.19) (16.0)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=43.0.1->evidently==0.7.19) (2.21)\nRequirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.12/site-packages (from httpx>=0.22->litestar>=2.19.0->evidently==0.7.19) (1.0.2)\nRequirement already satisfied: mdurl~=0.1 in /databricks/python3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13->evidently==0.7.19) (0.1.0)\nRequirement already satisfied: faker>=5.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from polyfactory>=2.6.3->litestar>=2.19.0->evidently==0.7.19) (40.1.2)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nPackages installed successfully\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run only once)\n",
    "%pip install evidently==0.7.19 pyyaml==6.0.2 pandas==2.2.3 numpy==2.2.1 azure-storage-file-datalake==12.20.0 azure-identity==1.19.0 plotly==5.24.1\n",
    "\n",
    "print(\"Packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de1f29f-0fba-4264-9e0a-377a69ad1074",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Add utils to path (adjust path as needed for Databricks)\n",
    "# For Databricks, you should upload utils as a library or use %run\n",
    "sys.path.append('/Workspace/Users/ashu.009kamboj@gmail.com/data-drift-evidently-ai')  # Update this path\n",
    "sys.path.append(str(Path.cwd().parent))  # For local testing\n",
    "\n",
    "# Import custom utilities\n",
    "from utils import ConfigManager, DriftDetector, ReportManager, DataLoader, setup_logger\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6559f937-bb6f-41b4-b3fb-d6ab1797f20e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "491ae32a-1265-4fc7-a3c0-36ca480200c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Using existing Spark session from Databricks\n  Spark version: 4.0.0\n\nSpark Configuration:\n"
     ]
    }
   ],
   "source": [
    "# Get or create Spark session\n",
    "# In Databricks, 'spark' is already available\n",
    "try:\n",
    "    # Verify spark session\n",
    "    spark\n",
    "    print(\"âœ“ Using existing Spark session from Databricks\")\n",
    "    print(f\"  Spark version: {spark.version}\")\n",
    "    #print(f\"  Application name: {spark.sparkContext.appName}\")\n",
    "except NameError:\n",
    "    # Create Spark session (for local testing)\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"DataDriftDetection\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    print(\"âœ“ Created new Spark session\")\n",
    "\n",
    "# Display Spark configuration\n",
    "print(\"\\nSpark Configuration:\")\n",
    "#print(f\"  Master: {spark.sparkContext.master}\")\n",
    "#print(f\"  Default Parallelism: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5550a8e0-3e2b-40c6-aa5e-48c4bba82fe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66ec9b27-228f-4404-92ac-15014bc6c332",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration loaded successfully\n  - Catalog: data_catalog\n  - Schema: outputs\n  - Tables to monitor: 3\n\nStatistical Tests:\n  - Numerical: ks, wasserstein\n  - Categorical: chisquare, jensenshannon\n"
     ]
    }
   ],
   "source": [
    "# Configuration file path (adjust as needed)\n",
    "CONFIG_PATH = '../config/drift_config.yaml'\n",
    "\n",
    "# For Databricks, use workspace path\n",
    "# CONFIG_PATH = '/Workspace/Repos/<your-repo>/data-drift-evidently-ai/config/drift_config.yaml'\n",
    "\n",
    "# Initialize configuration manager\n",
    "try:\n",
    "    config = ConfigManager(CONFIG_PATH)\n",
    "    print(\"âœ“ Configuration loaded successfully\")\n",
    "    print(f\"  - Catalog: {config.get_catalog_name()}\")\n",
    "    print(f\"  - Schema: {config.get_schema_name()}\")\n",
    "    print(f\"  - Tables to monitor: {len(config.get_tables())}\")\n",
    "    \n",
    "    # Display statistical tests configuration\n",
    "    print(f\"\\nStatistical Tests:\")\n",
    "    print(f\"  - Numerical: {', '.join(config.get_statistical_tests('numerical'))}\")\n",
    "    print(f\"  - Categorical: {', '.join(config.get_statistical_tests('categorical'))}\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error loading configuration: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afccf4e4-67d5-4714-a3c9-3f18cb1b6f28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - ================================================================================\nINFO - Data Drift Detection - PySpark Version\nINFO - ================================================================================\nINFO - Configuration loaded from: ../config/drift_config.yaml\nINFO - Spark Version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "logger = setup_logger(\n",
    "    log_level=config.get_log_level(),\n",
    "    logger_name='data_drift_pyspark',\n",
    "    adls_config=config.get_adls_config() if config.is_adls_output_enabled() else None\n",
    ")\n",
    "\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"Data Drift Detection - PySpark Version\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"Configuration loaded from: {CONFIG_PATH}\")\n",
    "logger.info(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ede19f-90e3-4751-bea6-7d64ebbf2f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d953488-e9f7-406a-afd9-7a03cb4d899a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - âœ“ Drift detector initialized\nINFO - âœ“ Report manager initialized\nINFO - âœ“ Data loader initialized with Spark session\n\nâœ“ All components initialized successfully\n\nReady to process tables using PySpark for large-scale drift detection\n"
     ]
    }
   ],
   "source": [
    "# Initialize drift detector\n",
    "drift_detector = DriftDetector(config)\n",
    "logger.info(\"âœ“ Drift detector initialized\")\n",
    "\n",
    "# Initialize report manager\n",
    "report_manager = ReportManager(config)\n",
    "logger.info(\"âœ“ Report manager initialized\")\n",
    "\n",
    "# Initialize data loader with Spark session\n",
    "data_loader = DataLoader(config, spark=spark)\n",
    "logger.info(\"âœ“ Data loader initialized with Spark session\")\n",
    "\n",
    "print(\"\\nâœ“ All components initialized successfully\")\n",
    "print(\"\\nReady to process tables using PySpark for large-scale drift detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d23b32fc-6845-4ec7-9692-4f13829cd22f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions for PySpark Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9a7425a-9ce0-4769-9749-44157c5dc549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def get_table_statistics(spark_df, table_name):\n",
    "    \"\"\"\n",
    "    Get basic statistics for a Spark DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        spark_df: Spark DataFrame\n",
    "        table_name: Name of the table\n",
    "    \n",
    "    Returns:\n",
    "        dict: Statistics dictionary\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'table_name': table_name,\n",
    "        'row_count': spark_df.count(),\n",
    "        'column_count': len(spark_df.columns),\n",
    "        'columns': spark_df.columns,\n",
    "        'size_mb': None  # Can be calculated if needed\n",
    "    }\n",
    "    \n",
    "    # Get data types distribution\n",
    "    dtypes_dict = dict(spark_df.dtypes)\n",
    "    numerical_cols = [col for col, dtype in dtypes_dict.items() \n",
    "                     if dtype in ['int', 'bigint', 'double', 'float', 'decimal']]\n",
    "    categorical_cols = [col for col, dtype in dtypes_dict.items() \n",
    "                       if dtype in ['string', 'boolean']]\n",
    "    \n",
    "    stats['numerical_columns'] = numerical_cols\n",
    "    stats['categorical_columns'] = categorical_cols\n",
    "    stats['numerical_count'] = len(numerical_cols)\n",
    "    stats['categorical_count'] = len(categorical_cols)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def optimize_spark_df_for_drift(spark_df, sampling_config):\n",
    "    \"\"\"\n",
    "    Optimize Spark DataFrame for drift detection by applying sampling if needed.\n",
    "    \n",
    "    Args:\n",
    "        spark_df: Input Spark DataFrame\n",
    "        sampling_config: Sampling configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "        Optimized Spark DataFrame\n",
    "    \"\"\"\n",
    "    if not sampling_config.get('enabled', False):\n",
    "        return spark_df\n",
    "    \n",
    "    method = sampling_config.get('method', 'fraction')\n",
    "    random_seed = sampling_config.get('random_seed', 42)\n",
    "    \n",
    "    if method == 'fraction':\n",
    "        fraction = sampling_config.get('fraction', 0.1)\n",
    "        sampled_df = spark_df.sample(withReplacement=False, fraction=fraction, seed=random_seed)\n",
    "        logger.info(f\"Applied sampling with fraction: {fraction}\")\n",
    "    else:  # fixed size\n",
    "        total_count = spark_df.count()\n",
    "        fixed_size = sampling_config.get('fixed_size', 10000)\n",
    "        if total_count > fixed_size:\n",
    "            fraction = fixed_size / total_count\n",
    "            sampled_df = spark_df.sample(withReplacement=False, fraction=fraction, seed=random_seed)\n",
    "            logger.info(f\"Applied sampling for fixed size: {fixed_size} from {total_count} rows\")\n",
    "        else:\n",
    "            sampled_df = spark_df\n",
    "    \n",
    "    return sampled_df\n",
    "\n",
    "print(\"âœ“ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2340027-582e-4419-8af6-651a296b7d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Process Each Table with PySpark\n",
    "\n",
    "Loop through configured tables and detect drift using distributed processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a8c9918-7b2f-47a7-a307-b211bf058cbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nProcessing 3 tables for drift detection using PySpark...\n================================================================================\nINFO - \n================================================================================\nINFO - Processing table 1/3: customer_data\nINFO - ================================================================================\nINFO - Loading data for table: customer_data\nComparing UC versions: 2 (reference) vs 3 (current)\nINFO - Data loaded - Reference: 10000 rows, Current: 10000 rows\nINFO - Running drift detection with Evidently AI...\nWARNING - âš ï¸  DRIFT DETECTED in customer_data\nWARNING -    Drifted columns (6): ['income', 'age', 'credit_score', 'data_version', 'account_type', 'region']\nINFO -    Total columns analyzed: 9\nINFO -    Numerical columns: 4\nINFO -    Categorical columns: 5\nINFO -    Drifted columns: 6\nINFO -    Drift share: 75.00%\nINFO - Saving reports for customer_data...\nINFO -    local_html: /Workspace/Users/ashu.009kamboj@gmail.com/data-drift-evidently-ai/reports/html/customer_data_20260120_165722_drift_report.html\nINFO -    local_json: /Workspace/Users/ashu.009kamboj@gmail.com/data-drift-evidently-ai/reports/json/customer_data_20260120_165722_drift_report.json\n\nâœ“ Completed: customer_data\n  âš ï¸  DRIFT DETECTED\n     - Drifted columns: 6/9\n     - Drift share: 75.00%\n     - Columns: income, age, credit_score, data_version, account_type...\n--------------------------------------------------------------------------------\nINFO - \n================================================================================\nINFO - Processing table 2/3: product_sales\nINFO - ================================================================================\nINFO - Loading data for table: product_sales\nComparing UC versions: 2 (reference) vs 3 (current)\nINFO - Data loaded - Reference: 50000 rows, Current: 50000 rows\nINFO - Running drift detection with Evidently AI...\nWARNING - âš ï¸  DRIFT DETECTED in product_sales\nWARNING -    Drifted columns (3): ['revenue', 'price', 'product_category']\nINFO -    Total columns analyzed: 4\nINFO -    Numerical columns: 2\nINFO -    Categorical columns: 2\nINFO -    Drifted columns: 3\nINFO -    Drift share: 75.00%\nINFO - Saving reports for product_sales...\nINFO -    local_html: /Workspace/Users/ashu.009kamboj@gmail.com/data-drift-evidently-ai/reports/html/product_sales_20260120_165727_drift_report.html\nINFO -    local_json: /Workspace/Users/ashu.009kamboj@gmail.com/data-drift-evidently-ai/reports/json/product_sales_20260120_165727_drift_report.json\n\nâœ“ Completed: product_sales\n  âš ï¸  DRIFT DETECTED\n     - Drifted columns: 3/4\n     - Drift share: 75.00%\n     - Columns: revenue, price, product_category\n--------------------------------------------------------------------------------\nINFO - \n================================================================================\nINFO - Processing table 3/3: user_behavior\nINFO - ================================================================================\nINFO - Loading data for table: user_behavior\nComparing UC versions: 2 (reference) vs 3 (current)\nINFO - Data loaded - Reference: 30000 rows, Current: 30000 rows\nINFO - Running drift detection with Evidently AI...\nWARNING - âš ï¸  DRIFT DETECTED in user_behavior\nWARNING -    Drifted columns (5): ['session_duration_minutes', 'pages_viewed', 'device_type', 'data_version', 'traffic_source']\nINFO -    Total columns analyzed: 9\nINFO -    Numerical columns: 3\nINFO -    Categorical columns: 6\nINFO -    Drifted columns: 5\nINFO -    Drift share: 62.50%\nINFO - Saving reports for user_behavior...\nINFO -    local_html: /Workspace/Users/ashu.009kamboj@gmail.com/data-drift-evidently-ai/reports/html/user_behavior_20260120_165733_drift_report.html\nINFO -    local_json: /Workspace/Users/ashu.009kamboj@gmail.com/data-drift-evidently-ai/reports/json/user_behavior_20260120_165733_drift_report.json\n\nâœ“ Completed: user_behavior\n  âš ï¸  DRIFT DETECTED\n     - Drifted columns: 5/9\n     - Drift share: 62.50%\n     - Columns: session_duration_minutes, pages_viewed, device_type, data_version, traffic_source\n--------------------------------------------------------------------------------\nINFO - \n================================================================================\nINFO - PySpark drift detection completed for all tables\nINFO - ================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Get list of tables from configuration\n",
    "tables = config.get_tables()\n",
    "results_summary = []\n",
    "\n",
    "# Get sampling configuration\n",
    "sampling_config = config.get_sampling_config()\n",
    "\n",
    "print(f\"\\nProcessing {len(tables)} tables for drift detection using PySpark...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, table_config in enumerate(tables, 1):\n",
    "    table_name = table_config['name']\n",
    "    columns = table_config.get('columns', 'all')\n",
    "    \n",
    "    logger.info(f\"\\n{'='*80}\")\n",
    "    logger.info(f\"Processing table {idx}/{len(tables)}: {table_name}\")\n",
    "    logger.info(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Load data versions using PySpark\n",
    "        logger.info(f\"Loading data for table: {table_name}\")\n",
    "        reference_data, current_data = data_loader.load_table_versions(\n",
    "            table_name=table_name,\n",
    "            use_spark=True  # Using PySpark for distributed loading\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Data loaded - Reference: {len(reference_data)} rows, Current: {len(current_data)} rows\")\n",
    "        \n",
    "        # Note: Data is already converted to Pandas by DataLoader\n",
    "        # This is necessary because Evidently AI works with Pandas DataFrames\n",
    "        \n",
    "        # Apply sampling if configured (already handled in DataLoader, but can be applied again if needed)\n",
    "        if sampling_config.get('enabled', False):\n",
    "            logger.info(\"Applying additional sampling for drift detection...\")\n",
    "            reference_data = drift_detector.apply_sampling(reference_data)\n",
    "            current_data = drift_detector.apply_sampling(current_data)\n",
    "        \n",
    "        # Detect drift\n",
    "        logger.info(f\"Running drift detection with Evidently AI...\")\n",
    "        report, drift_summary = drift_detector.detect_drift(\n",
    "            reference_data=reference_data,\n",
    "            current_data=current_data,\n",
    "            table_name=table_name,\n",
    "            columns=columns if columns != 'all' else None,\n",
    "        )\n",
    "        \n",
    "        # Log drift results\n",
    "        if drift_summary['dataset_drift']:\n",
    "            logger.warning(f\"âš ï¸  DRIFT DETECTED in {table_name}\")\n",
    "            logger.warning(f\"   Drifted columns ({drift_summary['num_drifted_columns']}): {drift_summary['drifted_columns']}\")\n",
    "        else:\n",
    "            logger.info(f\"âœ“ No significant drift detected in {table_name}\")\n",
    "        \n",
    "        logger.info(f\"   Total columns analyzed: {drift_summary['num_columns']}\")\n",
    "        logger.info(f\"   Numerical columns: {drift_summary['num_numerical_columns']}\")\n",
    "        logger.info(f\"   Categorical columns: {drift_summary['num_categorical_columns']}\")\n",
    "        logger.info(f\"   Drifted columns: {drift_summary['num_drifted_columns']}\")\n",
    "        logger.info(f\"   Drift share: {drift_summary['drift_share']:.2%}\")\n",
    "        \n",
    "        # Save reports\n",
    "        logger.info(f\"Saving reports for {table_name}...\")\n",
    "        saved_paths = report_manager.save_reports(\n",
    "            report=report,\n",
    "            drift_summary=drift_summary,\n",
    "            table_name=table_name\n",
    "        )\n",
    "        \n",
    "        for report_type, path in saved_paths.items():\n",
    "            logger.info(f\"   {report_type}: {path}\")\n",
    "        \n",
    "        # Add to summary\n",
    "        results_summary.append({\n",
    "            'table_name': table_name,\n",
    "            'total_columns': drift_summary['num_columns'],\n",
    "            'numerical_columns': drift_summary['num_numerical_columns'],\n",
    "            'categorical_columns': drift_summary['num_categorical_columns'],\n",
    "            'drifted_columns': drift_summary['num_drifted_columns'],\n",
    "            'drift_share': drift_summary['drift_share'],\n",
    "            'dataset_drift': drift_summary['dataset_drift'],\n",
    "            'drifted_column_names': ', '.join(drift_summary['drifted_columns']),\n",
    "            'report_paths': saved_paths\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nâœ“ Completed: {table_name}\")\n",
    "        if drift_summary['dataset_drift']:\n",
    "            print(f\"  âš ï¸  DRIFT DETECTED\")\n",
    "            print(f\"     - Drifted columns: {drift_summary['num_drifted_columns']}/{drift_summary['num_columns']}\")\n",
    "            print(f\"     - Drift share: {drift_summary['drift_share']:.2%}\")\n",
    "            print(f\"     - Columns: {', '.join(drift_summary['drifted_columns'][:5])}\" + \n",
    "                  (\"...\" if len(drift_summary['drifted_columns']) > 5 else \"\"))\n",
    "        else:\n",
    "            print(f\"  âœ“ No significant drift detected\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âœ— Error processing table {table_name}: {e}\", exc_info=True)\n",
    "        print(f\"\\nâœ— Error processing {table_name}: {e}\")\n",
    "        results_summary.append({\n",
    "            'table_name': table_name,\n",
    "            'error': str(e)\n",
    "        })\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"PySpark drift detection completed for all tables\")\n",
    "logger.info(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a4ee402-2ba4-4198-8128-50b10dee9454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Detailed Summary Report\n",
    "\n",
    "Display comprehensive drift detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b229464c-8b65-4830-b5d6-4dc2efe22553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nDRIFT DETECTION SUMMARY (PySpark Version)\n================================================================================\n\nâœ“ Successfully processed 3 tables using PySpark\n\n        Table  Total Cols  Numerical  Categorical  Drifted Drift %   Status\ncustomer_data           9          4            5        6  75.00% âš ï¸ DRIFT\nproduct_sales           4          2            2        3  75.00% âš ï¸ DRIFT\nuser_behavior           9          3            6        5  62.50% âš ï¸ DRIFT\n\n================================================================================\nDrifted Column Details:\n================================================================================\n\ncustomer_data:\n  Drifted columns: income, age, credit_score, data_version, account_type, region\n\nproduct_sales:\n  Drifted columns: revenue, price, product_category\n\nuser_behavior:\n  Drifted columns: session_duration_minutes, pages_viewed, device_type, data_version, traffic_source\n\n================================================================================\nOverall Statistics:\n  - Total tables processed: 3\n  - Tables with drift: 3\n  - Tables without drift: 0\n  - Total columns analyzed: 22\n  - Total drifted columns: 14\n  - Overall drift rate: 63.64%\n================================================================================\n\n"
     ]
    }
   ],
   "source": [
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(results_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DRIFT DETECTION SUMMARY (PySpark Version)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'error' in summary_df.columns:\n",
    "    # Show tables with errors\n",
    "    error_tables = summary_df[summary_df['error'].notna()]\n",
    "    if not error_tables.empty:\n",
    "        print(\"\\nâš ï¸  Tables with errors:\")\n",
    "        for _, row in error_tables.iterrows():\n",
    "            print(f\"  - {row['table_name']}: {row['error']}\")\n",
    "        print()\n",
    "else:\n",
    "    # Ensure 'error' column exists (add it if missing)\n",
    "    summary_df['error'] = None  # or pd.NA\n",
    "    \n",
    "# Show tables without errors\n",
    "success_df = summary_df[~summary_df['table_name'].isin(summary_df[summary_df.get('error', pd.Series()).notna()]['table_name'])]\n",
    "\n",
    "if not success_df.empty:\n",
    "    print(f\"\\nâœ“ Successfully processed {len(success_df)} tables using PySpark\\n\")\n",
    "    \n",
    "    # Display summary table\n",
    "    display_columns = ['table_name', 'total_columns', 'numerical_columns', \n",
    "                      'categorical_columns', 'drifted_columns', 'drift_share', 'dataset_drift']\n",
    "    display_df = success_df[display_columns].copy()\n",
    "    display_df['drift_share'] = display_df['drift_share'].apply(lambda x: f\"{x:.2%}\")\n",
    "    display_df['drift_status'] = display_df['dataset_drift'].apply(lambda x: 'âš ï¸ DRIFT' if x else 'âœ“ OK')\n",
    "    display_df = display_df.drop('dataset_drift', axis=1)\n",
    "    display_df.columns = ['Table', 'Total Cols', 'Numerical', 'Categorical', \n",
    "                         'Drifted', 'Drift %', 'Status']\n",
    "    \n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    # Show drifted columns for tables with drift\n",
    "    drifted_tables = success_df[success_df['dataset_drift'] == True]\n",
    "    if not drifted_tables.empty:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"Drifted Column Details:\")\n",
    "        print(f\"{'='*80}\")\n",
    "        for _, row in drifted_tables.iterrows():\n",
    "            print(f\"\\n{row['table_name']}:\")\n",
    "            print(f\"  Drifted columns: {row['drifted_column_names']}\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_drifted = (display_df['Status'] == 'âš ï¸ DRIFT').sum()\n",
    "    total_ok = (display_df['Status'] == 'âœ“ OK').sum()\n",
    "    total_cols = display_df['Total Cols'].sum()\n",
    "    total_drifted_cols = success_df['drifted_columns'].sum()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Overall Statistics:\")\n",
    "    print(f\"  - Total tables processed: {len(display_df)}\")\n",
    "    print(f\"  - Tables with drift: {total_drifted}\")\n",
    "    print(f\"  - Tables without drift: {total_ok}\")\n",
    "    print(f\"  - Total columns analyzed: {total_cols}\")\n",
    "    print(f\"  - Total drifted columns: {total_drifted_cols}\")\n",
    "    if total_cols > 0:\n",
    "        print(f\"  - Overall drift rate: {(total_drifted_cols/total_cols)*100:.2f}%\")\n",
    "    print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b688f48a-91b7-42f7-ac7f-e374ddca0a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Access Reports\n",
    "\n",
    "Information on accessing generated reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bd8f3cd-a4cb-48d2-a0fe-31d98becb064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nReport Locations:\n================================================================================\n\n\uD83D\uDCC1 Local Reports:\n   Path: reports\n   - HTML reports: reports/html/\n   - JSON reports: reports/json/\n\n================================================================================\n\nTo view reports:\n  1. HTML Reports (local): Open in web browser for interactive visualization\n  2. JSON Reports (local): Use for programmatic analysis or monitoring integration\n  3. ADLS (if enabled): Access via abfss path in your storage account\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Report Locations:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Local reports are saved by default for testing\n",
    "print(f\"\\n\uD83D\uDCC1 Local Reports:\")\n",
    "print(f\"   Path: reports\")\n",
    "print(f\"   - HTML reports: reports/html/\")\n",
    "print(f\"   - JSON reports: reports/json/\")\n",
    "\n",
    "if config.is_adls_output_enabled():\n",
    "    adls_config = config.get_adls_config()\n",
    "    print(f\"\\nâ˜ï¸  ADLS Reports:\")\n",
    "    print(f\"   Container: {adls_config['container']}\")\n",
    "    print(f\"   Base Path: {adls_config['base_path']}\")\n",
    "    print(f\"   Storage Account: {adls_config['storage_account']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nTo view reports:\")\n",
    "print(\"  1. HTML Reports (local): Open in web browser for interactive visualization\")\n",
    "print(\"  2. JSON Reports (local): Use for programmatic analysis or monitoring integration\")\n",
    "print(\"  3. ADLS (if enabled): Access via abfss path in your storage account\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66d4abd0-003e-475e-9166-f36d0f85540e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "### PySpark Advantages\n",
    "- **Scalability**: Handles large datasets efficiently using distributed processing\n",
    "- **Performance**: Leverages Spark's in-memory computation and optimization\n",
    "- **Integration**: Seamless integration with Databricks Unity Catalog\n",
    "- **Resource Management**: Better memory management for large-scale operations\n",
    "\n",
    "### Key Differences from Python Version\n",
    "1. **Data Loading**: Uses Spark for distributed data loading\n",
    "2. **Processing**: Leverages Spark's parallel processing capabilities\n",
    "3. **Memory**: More efficient memory usage for large datasets\n",
    "4. **Sampling**: Can apply distributed sampling before conversion to Pandas\n",
    "\n",
    "### Next Steps\n",
    "1. **Review Reports**: Analyze detailed drift reports for affected tables\n",
    "2. **Investigate Causes**: Identify root causes of detected drift\n",
    "3. **Tune Configuration**: Adjust thresholds and tests based on results\n",
    "4. **Schedule Jobs**: Set up Databricks jobs for automated monitoring\n",
    "5. **Set Alerts**: Configure alerting based on drift detection results\n",
    "6. **Scale Up**: Increase cluster size for larger datasets if needed\n",
    "\n",
    "### Production Tips\n",
    "- Use Databricks job scheduling for regular drift monitoring\n",
    "- Enable ADLS output for centralized report storage\n",
    "- Configure appropriate sampling for very large tables (>100M rows)\n",
    "- Use Delta Lake time travel for precise version comparison\n",
    "- Set up downstream alerts and notifications\n",
    "- Monitor Spark job performance and optimize as needed"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_drift_detection_pyspark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}