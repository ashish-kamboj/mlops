{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dba3a71-f33a-438a-b5cf-6ed1599ee6f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evidently==0.7.19 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (0.7.19)\n",
      "Requirement already satisfied: pyyaml==6.0.2 in /databricks/python3/lib/python3.12/site-packages (6.0.2)\n",
      "Requirement already satisfied: pandas==2.2.3 in /databricks/python3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy==2.2.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: azure-storage-file-datalake==12.20.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (12.20.0)\n",
      "Requirement already satisfied: azure-identity==1.19.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (1.19.0)\n",
      "Requirement already satisfied: plotly==5.24.1 in /databricks/python3/lib/python3.12/site-packages (5.24.1)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (2025.1.31)\n",
      "Requirement already satisfied: cryptography>=43.0.1 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (43.0.3)\n",
      "Requirement already satisfied: deprecation>=2.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (2.1.0)\n",
      "Requirement already satisfied: dynaconf>=3.2.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (3.2.12)\n",
      "Requirement already satisfied: fsspec>=2024.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (2026.1.0)\n",
      "Requirement already satisfied: iterative-telemetry>=0.0.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (0.0.10)\n",
      "Requirement already satisfied: litestar>=2.19.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (2.19.0)\n",
      "Requirement already satisfied: nltk>=3.6.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (3.9.2)\n",
      "Requirement already satisfied: opentelemetry-proto>=1.25.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (1.39.1)\n",
      "Requirement already satisfied: pydantic>=1.10.16 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (2.10.6)\n",
      "Requirement already satisfied: requests>=2.32.0 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (2.32.3)\n",
      "Requirement already satisfied: rich>=13 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (13.9.4)\n",
      "Requirement already satisfied: scikit-learn>=1.1.1 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (1.15.1)\n",
      "Requirement already satisfied: statsmodels>=0.12.2 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (0.14.4)\n",
      "Requirement already satisfied: typer>=0.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (0.21.1)\n",
      "Requirement already satisfied: typing-inspect>=0.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (0.9.0)\n",
      "Requirement already satisfied: ujson>=5.4.0 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (5.10.0)\n",
      "Requirement already satisfied: urllib3>=1.26.19 in /databricks/python3/lib/python3.12/site-packages (from evidently==0.7.19) (2.3.0)\n",
      "Requirement already satisfied: uuid6>=2024.7.10 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (2025.0.1)\n",
      "Requirement already satisfied: uvicorn>=0.22.0 in /databricks/python3/lib/python3.12/site-packages (from uvicorn[standard]>=0.22.0->evidently==0.7.19) (0.34.2)\n",
      "Requirement already satisfied: watchdog>=3.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from evidently==0.7.19) (6.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /databricks/python3/lib/python3.12/site-packages (from pandas==2.2.3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas==2.2.3) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas==2.2.3) (2024.1)\n",
      "Requirement already satisfied: azure-core>=1.30.0 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake==12.20.0) (1.34.0)\n",
      "Requirement already satisfied: azure-storage-blob>=12.25.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from azure-storage-file-datalake==12.20.0) (12.28.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake==12.20.0) (4.12.2)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake==12.20.0) (0.6.1)\n",
      "Requirement already satisfied: msal>=1.30.0 in /databricks/python3/lib/python3.12/site-packages (from azure-identity==1.19.0) (1.32.3)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from azure-identity==1.19.0) (1.3.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /databricks/python3/lib/python3.12/site-packages (from plotly==5.24.1) (9.0.0)\n",
      "Requirement already satisfied: packaging in /databricks/python3/lib/python3.12/site-packages (from plotly==5.24.1) (24.1)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core>=1.30.0->azure-storage-file-datalake==12.20.0) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.12/site-packages (from cryptography>=43.0.1->evidently==0.7.19) (1.17.1)\n",
      "Requirement already satisfied: appdirs in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from iterative-telemetry>=0.0.5->evidently==0.7.19) (1.4.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from iterative-telemetry>=0.0.5->evidently==0.7.19) (3.18.0)\n",
      "Requirement already satisfied: distro in /usr/lib/python3/dist-packages (from iterative-telemetry>=0.0.5->evidently==0.7.19) (1.9.0)\n",
      "Requirement already satisfied: anyio>=3 in /databricks/python3/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (4.6.2)\n",
      "Requirement already satisfied: click in /databricks/python3/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (8.1.7)\n",
      "Requirement already satisfied: httpx>=0.22 in /databricks/python3/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (0.27.0)\n",
      "Requirement already satisfied: litestar-htmx>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (0.5.0)\n",
      "Requirement already satisfied: msgspec>=0.18.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (0.20.0)\n",
      "Requirement already satisfied: multidict>=6.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (6.7.0)\n",
      "Requirement already satisfied: multipart>=1.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (1.3.0)\n",
      "Requirement already satisfied: polyfactory>=2.6.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (3.2.0)\n",
      "Requirement already satisfied: rich-click in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (1.9.5)\n",
      "Requirement already satisfied: sniffio>=1.3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from litestar>=2.19.0->evidently==0.7.19) (1.3.1)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /databricks/python3/lib/python3.12/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity==1.19.0) (2.10.1)\n",
      "Requirement already satisfied: joblib in /databricks/python3/lib/python3.12/site-packages (from nltk>=3.6.7->evidently==0.7.19) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from nltk>=3.6.7->evidently==0.7.19) (2026.1.15)\n",
      "Requirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from nltk>=3.6.7->evidently==0.7.19) (4.67.1)\n",
      "Requirement already satisfied: protobuf<7.0,>=5.0 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-proto>=1.25.0->evidently==0.7.19) (5.29.4)\n",
      "Requirement already satisfied: pyarrow>=10.0.1 in /databricks/python3/lib/python3.12/site-packages (from pandas[parquet]>=1.3.5->evidently==0.7.19) (19.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic>=1.10.16->evidently==0.7.19) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /databricks/python3/lib/python3.12/site-packages (from pydantic>=1.10.16->evidently==0.7.19) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.32.0->evidently==0.7.19) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.32.0->evidently==0.7.19) (3.7)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /databricks/python3/lib/python3.12/site-packages (from rich>=13->evidently==0.7.19) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /databricks/python3/lib/python3.12/site-packages (from rich>=13->evidently==0.7.19) (2.15.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn>=1.1.1->evidently==0.7.19) (3.5.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /databricks/python3/lib/python3.12/site-packages (from statsmodels>=0.12.2->evidently==0.7.19) (1.0.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from typer>=0.3->evidently==0.7.19) (1.5.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from typing-inspect>=0.9.0->evidently==0.7.19) (1.0.0)\n",
      "Requirement already satisfied: h11>=0.8 in /databricks/python3/lib/python3.12/site-packages (from uvicorn>=0.22.0->uvicorn[standard]>=0.22.0->evidently==0.7.19) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from uvicorn[standard]>=0.22.0->evidently==0.7.19) (0.7.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from uvicorn[standard]>=0.22.0->evidently==0.7.19) (1.2.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from uvicorn[standard]>=0.22.0->evidently==0.7.19) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from uvicorn[standard]>=0.22.0->evidently==0.7.19) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from uvicorn[standard]>=0.22.0->evidently==0.7.19) (16.0)\n",
      "Requirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=43.0.1->evidently==0.7.19) (2.21)\n",
      "Requirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.12/site-packages (from httpx>=0.22->litestar>=2.19.0->evidently==0.7.19) (1.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /databricks/python3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13->evidently==0.7.19) (0.1.0)\n",
      "Requirement already satisfied: faker>=5.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a4eccd1-c069-47c0-afac-481a6de23257/lib/python3.12/site-packages (from polyfactory>=2.6.3->litestar>=2.19.0->evidently==0.7.19) (40.1.2)\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Packages installed successfully\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run only once)\n",
    "%pip install evidently==0.7.19 pyyaml==6.0.2 pandas==2.2.3 numpy==2.2.1 azure-storage-file-datalake==12.20.0 azure-identity==1.19.0 plotly==5.24.1\n",
    "\n",
    "print(\"Packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de1f29f-0fba-4264-9e0a-377a69ad1074",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√¢≈ì‚Äú All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Add utils to path (adjust path as needed for Databricks)\n",
    "# For Databricks, you should upload utils as a library or use %run\n",
    "sys.path.append('/Workspace/Users/<<USER_ID>>/data-drift-evidently-ai')  # Update this path\n",
    "sys.path.append(str(Path.cwd().parent))  # For local testing\n",
    "\n",
    "# Import custom utilities\n",
    "from utils import ConfigManager, DriftDetector, ReportManager, DataLoader, setup_logger\n",
    "\n",
    "print(\"√¢≈ì‚Äú All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6559f937-bb6f-41b4-b3fb-d6ab1797f20e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "491ae32a-1265-4fc7-a3c0-36ca480200c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√¢≈ì‚Äú Using existing Spark session from Databricks\n",
      "  Spark version: 4.0.0\n",
      "\n",
      "Spark Configuration:\n"
     ]
    }
   ],
   "source": [
    "# Get or create Spark session\n",
    "# In Databricks, 'spark' is already available\n",
    "try:\n",
    "    # Verify spark session\n",
    "    spark\n",
    "    print(\"√¢≈ì‚Äú Using existing Spark session from Databricks\")\n",
    "    print(f\"  Spark version: {spark.version}\")\n",
    "    #print(f\"  Application name: {spark.sparkContext.appName}\")\n",
    "except NameError:\n",
    "    # Create Spark session (for local testing)\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"DataDriftDetection\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    print(\"√¢≈ì‚Äú Created new Spark session\")\n",
    "\n",
    "# Display Spark configuration\n",
    "print(\"\\nSpark Configuration:\")\n",
    "#print(f\"  Master: {spark.sparkContext.master}\")\n",
    "#print(f\"  Default Parallelism: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5550a8e0-3e2b-40c6-aa5e-48c4bba82fe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66ec9b27-228f-4404-92ac-15014bc6c332",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√¢≈ì‚Äú Configuration loaded successfully\n",
      "  - Catalog: data_catalog\n",
      "  - Schema: outputs\n",
      "  - Tables to monitor: 3\n",
      "\n",
      "Statistical Tests:\n",
      "  - Numerical: ks, wasserstein\n",
      "  - Categorical: chisquare, jensenshannon\n"
     ]
    }
   ],
   "source": [
    "# Configuration file path (adjust as needed)\n",
    "CONFIG_PATH = '../config/drift_config.yaml'\n",
    "\n",
    "# For Databricks, use workspace path\n",
    "# CONFIG_PATH = '/Workspace/Repos/<your-repo>/data-drift-evidently-ai/config/drift_config.yaml'\n",
    "\n",
    "# Initialize configuration manager\n",
    "try:\n",
    "    config = ConfigManager(CONFIG_PATH)\n",
    "    print(\"√¢≈ì‚Äú Configuration loaded successfully\")\n",
    "    print(f\"  - Catalog: {config.get_catalog_name()}\")\n",
    "    print(f\"  - Schema: {config.get_schema_name()}\")\n",
    "    print(f\"  - Tables to monitor: {len(config.get_tables())}\")\n",
    "    \n",
    "    # Display statistical tests configuration\n",
    "    print(f\"\\nStatistical Tests:\")\n",
    "    print(f\"  - Numerical: {', '.join(config.get_statistical_tests('numerical'))}\")\n",
    "    print(f\"  - Categorical: {', '.join(config.get_statistical_tests('categorical'))}\")\n",
    "except Exception as e:\n",
    "    print(f\"√¢≈ì‚Äî Error loading configuration: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afccf4e4-67d5-4714-a3c9-3f18cb1b6f28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - ================================================================================\n",
      "INFO - Data Drift Detection - PySpark Version\n",
      "INFO - ================================================================================\n",
      "INFO - Configuration loaded from: ../config/drift_config.yaml\n",
      "INFO - Spark Version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "logger = setup_logger(\n",
    "    log_level=config.get_log_level(),\n",
    "    logger_name='data_drift_pyspark',\n",
    "    adls_config=config.get_adls_config() if config.is_adls_output_enabled() else None\n",
    ")\n",
    "\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"Data Drift Detection - PySpark Version\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"Configuration loaded from: {CONFIG_PATH}\")\n",
    "logger.info(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ede19f-90e3-4751-bea6-7d64ebbf2f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d953488-e9f7-406a-afd9-7a03cb4d899a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - √¢≈ì‚Äú Drift detector initialized\n",
      "INFO - √¢≈ì‚Äú Report manager initialized\n",
      "INFO - √¢≈ì‚Äú Data loader initialized with Spark session\n",
      "\n",
      "√¢≈ì‚Äú All components initialized successfully\n",
      "\n",
      "Ready to process tables using PySpark for large-scale drift detection\n"
     ]
    }
   ],
   "source": [
    "# Initialize drift detector\n",
    "drift_detector = DriftDetector(config)\n",
    "logger.info(\"√¢≈ì‚Äú Drift detector initialized\")\n",
    "\n",
    "# Initialize report manager\n",
    "report_manager = ReportManager(config)\n",
    "logger.info(\"√¢≈ì‚Äú Report manager initialized\")\n",
    "\n",
    "# Initialize data loader with Spark session\n",
    "data_loader = DataLoader(config, spark=spark)\n",
    "logger.info(\"√¢≈ì‚Äú Data loader initialized with Spark session\")\n",
    "\n",
    "print(\"\\n√¢≈ì‚Äú All components initialized successfully\")\n",
    "print(\"\\nReady to process tables using PySpark for large-scale drift detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d23b32fc-6845-4ec7-9692-4f13829cd22f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions for PySpark Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9a7425a-9ce0-4769-9749-44157c5dc549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√¢≈ì‚Äú Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def get_table_statistics(spark_df, table_name):\n",
    "    \"\"\"\n",
    "    Get basic statistics for a Spark DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        spark_df: Spark DataFrame\n",
    "        table_name: Name of the table\n",
    "    \n",
    "    Returns:\n",
    "        dict: Statistics dictionary\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'table_name': table_name,\n",
    "        'row_count': spark_df.count(),\n",
    "        'column_count': len(spark_df.columns),\n",
    "        'columns': spark_df.columns,\n",
    "        'size_mb': None  # Can be calculated if needed\n",
    "    }\n",
    "    \n",
    "    # Get data types distribution\n",
    "    dtypes_dict = dict(spark_df.dtypes)\n",
    "    numerical_cols = [col for col, dtype in dtypes_dict.items() \n",
    "                     if dtype in ['int', 'bigint', 'double', 'float', 'decimal']]\n",
    "    categorical_cols = [col for col, dtype in dtypes_dict.items() \n",
    "                       if dtype in ['string', 'boolean']]\n",
    "    \n",
    "    stats['numerical_columns'] = numerical_cols\n",
    "    stats['categorical_columns'] = categorical_cols\n",
    "    stats['numerical_count'] = len(numerical_cols)\n",
    "    stats['categorical_count'] = len(categorical_cols)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def optimize_spark_df_for_drift(spark_df, sampling_config):\n",
    "    \"\"\"\n",
    "    Optimize Spark DataFrame for drift detection by applying sampling if needed.\n",
    "    \n",
    "    Args:\n",
    "        spark_df: Input Spark DataFrame\n",
    "        sampling_config: Sampling configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "        Optimized Spark DataFrame\n",
    "    \"\"\"\n",
    "    if not sampling_config.get('enabled', False):\n",
    "        return spark_df\n",
    "    \n",
    "    method = sampling_config.get('method', 'fraction')\n",
    "    random_seed = sampling_config.get('random_seed', 42)\n",
    "    \n",
    "    if method == 'fraction':\n",
    "        fraction = sampling_config.get('fraction', 0.1)\n",
    "        sampled_df = spark_df.sample(withReplacement=False, fraction=fraction, seed=random_seed)\n",
    "        logger.info(f\"Applied sampling with fraction: {fraction}\")\n",
    "    else:  # fixed size\n",
    "        total_count = spark_df.count()\n",
    "        fixed_size = sampling_config.get('fixed_size', 10000)\n",
    "        if total_count > fixed_size:\n",
    "            fraction = fixed_size / total_count\n",
    "            sampled_df = spark_df.sample(withReplacement=False, fraction=fraction, seed=random_seed)\n",
    "            logger.info(f\"Applied sampling for fixed size: {fixed_size} from {total_count} rows\")\n",
    "        else:\n",
    "            sampled_df = spark_df\n",
    "    \n",
    "    return sampled_df\n",
    "\n",
    "print(\"√¢≈ì‚Äú Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2340027-582e-4419-8af6-651a296b7d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Process Each Table with PySpark\n",
    "\n",
    "Loop through configured tables and detect drift using distributed processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a8c9918-7b2f-47a7-a307-b211bf058cbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 3 tables for drift detection using PySpark...\n",
      "================================================================================\n",
      "INFO - \n",
      "================================================================================\n",
      "INFO - Processing table 1/3: customer_data\n",
      "INFO - ================================================================================\n",
      "INFO - Loading data for table: customer_data\n",
      "Comparing UC versions: 2 (reference) vs 3 (current)\n",
      "INFO - Data loaded - Reference: 10000 rows, Current: 10000 rows\n",
      "INFO - Running drift detection with Evidently AI...\n",
      "WARNING - √¢≈° √Ø¬∏¬è  DRIFT DETECTED in customer_data\n",
      "WARNING -    Drifted columns (6): ['income', 'age', 'credit_score', 'data_version', 'account_type', 'region']\n",
      "INFO -    Total columns analyzed: 9\n",
      "INFO -    Numerical columns: 4\n",
      "INFO -    Categorical columns: 5\n",
      "INFO -    Drifted columns: 6\n",
      "INFO -    Drift share: 75.00%\n",
      "INFO - Saving reports for customer_data...\n",
      "INFO -    local_html: /Workspace/Users/ashu.009kamboj@gmail.com/data-drift-evidently-ai/reports/html/customer_data_20260120_165722_drift_report.html\n",
      "INFO -    local_json: /Workspace/Users/ashu.009kamboj@gmail.com/data-drift-evidently-ai/reports/json/customer_data_20260120_165722_drift_report.json\n",
      "\n",
      "√¢≈ì‚Äú Completed: customer_data\n",
      "  √¢≈° √Ø¬∏¬è  DRIFT DETECTED\n",
      "     - Drifted columns: 6/9\n",
      "     - Drift share: 75.00%\n",
      "     - Columns: income, age, credit_score, data_version, account_type...\n",
      "--------------------------------------------------------------------------------\n",
      "INFO - \n",
      "================================================================================\n",
      "INFO - Processing table 2/3: product_sales\n",
      "INFO - ================================================================================\n",
      "INFO - Loading data for table: product_sales\n",
      "Comparing UC versions: 2 (reference) vs 3 (current)\n",
      "INFO - Data loaded - Reference: 50000 rows, Current: 50000 rows\n",
      "INFO - Running drift detection with Evidently AI...\n",
      "WARNING - √¢≈° √Ø¬∏¬è  DRIFT DETECTED in product_sales\n",
      "WARNING -    Drifted columns (3): ['revenue', 'price', 'product_category']\n",
      "INFO -    Total columns analyzed: 4\n",
      "INFO -    Numerical columns: 2\n",
      "INFO -    Categorical columns: 2\n",
      "INFO -    Drifted columns: 3\n",
      "INFO -    Drift share: 75.00%\n",
      "INFO - Saving reports for product_sales...\n",
      "INFO -    local_html: /Workspace/Users/ashu.009kamboj@gmail.com/data-drift-evidently-ai/reports/html/product_sales_20260120_165727_drift_report.html\n",
      "INFO -    local_json: /Workspace/Users/ashu.009kamboj@gmail.com/data-drift-evidently-ai/reports/json/product_sales_20260120_165727_drift_report.json\n",
      "\n",
      "√¢≈ì‚Äú Completed: product_sales\n",
      "  √¢≈° √Ø¬∏¬è  DRIFT DETECTED\n",
      "     - Drifted columns: 3/4\n",
      "     - Drift share: 75.00%\n",
      "     - Columns: revenue, price, product_category\n",
      "--------------------------------------------------------------------------------\n",
      "INFO - \n",
      "================================================================================\n",
      "INFO - Processing table 3/3: user_behavior\n",
      "INFO - ================================================================================\n",
      "INFO - Loading data for table: user_behavior\n",
      "Comparing UC versions: 2 (reference) vs 3 (current)\n",
      "INFO - Data loaded - Reference: 30000 rows, Current: 30000 rows\n",
      "INFO - Running drift detection with Evidently AI...\n",
      "WARNING - √¢≈° √Ø¬∏¬è  DRIFT DETECTED in user_behavior\n",
      "WARNING -    Drifted columns (5): ['session_duration_minutes', 'pages_viewed', 'device_type', 'data_version', 'traffic_source']\n",
      "INFO -    Total columns analyzed: 9\n",
      "INFO -    Numerical columns: 3\n",
      "INFO -    Categorical columns: 6\n",
      "INFO -    Drifted columns: 5\n",
      "INFO -    Drift share: 62.50%\n",
      "INFO - Saving reports for user_behavior...\n",
      "INFO -    local_html: /Workspace/Users/ashu.009kamboj@gmail.com/data-drift-evidently-ai/reports/html/user_behavior_20260120_165733_drift_report.html\n",
      "INFO -    local_json: /Workspace/Users/ashu.009kamboj@gmail.com/data-drift-evidently-ai/reports/json/user_behavior_20260120_165733_drift_report.json\n",
      "\n",
      "√¢≈ì‚Äú Completed: user_behavior\n",
      "  √¢≈° √Ø¬∏¬è  DRIFT DETECTED\n",
      "     - Drifted columns: 5/9\n",
      "     - Drift share: 62.50%\n",
      "     - Columns: session_duration_minutes, pages_viewed, device_type, data_version, traffic_source\n",
      "--------------------------------------------------------------------------------\n",
      "INFO - \n",
      "================================================================================\n",
      "INFO - PySpark drift detection completed for all tables\n",
      "INFO - ================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Get list of tables from configuration\n",
    "tables = config.get_tables()\n",
    "results_summary = []\n",
    "\n",
    "# Get sampling configuration\n",
    "sampling_config = config.get_sampling_config()\n",
    "\n",
    "print(f\"\\nProcessing {len(tables)} tables for drift detection using PySpark...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, table_config in enumerate(tables, 1):\n",
    "    table_name = table_config['name']\n",
    "    columns = table_config.get('columns', 'all')\n",
    "    \n",
    "    logger.info(f\"\\n{'='*80}\")\n",
    "    logger.info(f\"Processing table {idx}/{len(tables)}: {table_name}\")\n",
    "    logger.info(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Load data versions using PySpark\n",
    "        logger.info(f\"Loading data for table: {table_name}\")\n",
    "        reference_data, current_data = data_loader.load_table_versions(\n",
    "            table_name=table_name,\n",
    "            use_spark=True  # Using PySpark for distributed loading\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Data loaded - Reference: {len(reference_data)} rows, Current: {len(current_data)} rows\")\n",
    "        \n",
    "        # Note: Data is already converted to Pandas by DataLoader\n",
    "        # This is necessary because Evidently AI works with Pandas DataFrames\n",
    "        \n",
    "        # Apply sampling if configured (already handled in DataLoader, but can be applied again if needed)\n",
    "        if sampling_config.get('enabled', False):\n",
    "            logger.info(\"Applying additional sampling for drift detection...\")\n",
    "            reference_data = drift_detector.apply_sampling(reference_data)\n",
    "            current_data = drift_detector.apply_sampling(current_data)\n",
    "        \n",
    "        # Detect drift\n",
    "        logger.info(f\"Running drift detection with Evidently AI...\")\n",
    "        report, drift_summary = drift_detector.detect_drift(\n",
    "            reference_data=reference_data,\n",
    "            current_data=current_data,\n",
    "            table_name=table_name,\n",
    "            columns=columns if columns != 'all' else None,\n",
    "        )\n",
    "        \n",
    "        # Log drift results\n",
    "        if drift_summary['dataset_drift']:\n",
    "            logger.warning(f\"√¢≈° √Ø¬∏¬è  DRIFT DETECTED in {table_name}\")\n",
    "            logger.warning(f\"   Drifted columns ({drift_summary['num_drifted_columns']}): {drift_summary['drifted_columns']}\")\n",
    "        else:\n",
    "            logger.info(f\"√¢≈ì‚Äú No significant drift detected in {table_name}\")\n",
    "        \n",
    "        logger.info(f\"   Total columns analyzed: {drift_summary['num_columns']}\")\n",
    "        logger.info(f\"   Numerical columns: {drift_summary['num_numerical_columns']}\")\n",
    "        logger.info(f\"   Categorical columns: {drift_summary['num_categorical_columns']}\")\n",
    "        logger.info(f\"   Drifted columns: {drift_summary['num_drifted_columns']}\")\n",
    "        logger.info(f\"   Drift share: {drift_summary['drift_share']:.2%}\")\n",
    "        \n",
    "        # Save reports\n",
    "        logger.info(f\"Saving reports for {table_name}...\")\n",
    "        saved_paths = report_manager.save_reports(\n",
    "            report=report,\n",
    "            drift_summary=drift_summary,\n",
    "            table_name=table_name\n",
    "        )\n",
    "        \n",
    "        for report_type, path in saved_paths.items():\n",
    "            logger.info(f\"   {report_type}: {path}\")\n",
    "        \n",
    "        # Add to summary\n",
    "        results_summary.append({\n",
    "            'table_name': table_name,\n",
    "            'total_columns': drift_summary['num_columns'],\n",
    "            'numerical_columns': drift_summary['num_numerical_columns'],\n",
    "            'categorical_columns': drift_summary['num_categorical_columns'],\n",
    "            'drifted_columns': drift_summary['num_drifted_columns'],\n",
    "            'drift_share': drift_summary['drift_share'],\n",
    "            'dataset_drift': drift_summary['dataset_drift'],\n",
    "            'drifted_column_names': ', '.join(drift_summary['drifted_columns']),\n",
    "            'report_paths': saved_paths\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n√¢≈ì‚Äú Completed: {table_name}\")\n",
    "        if drift_summary['dataset_drift']:\n",
    "            print(f\"  √¢≈° √Ø¬∏¬è  DRIFT DETECTED\")\n",
    "            print(f\"     - Drifted columns: {drift_summary['num_drifted_columns']}/{drift_summary['num_columns']}\")\n",
    "            print(f\"     - Drift share: {drift_summary['drift_share']:.2%}\")\n",
    "            print(f\"     - Columns: {', '.join(drift_summary['drifted_columns'][:5])}\" + \n",
    "                  (\"...\" if len(drift_summary['drifted_columns']) > 5 else \"\"))\n",
    "        else:\n",
    "            print(f\"  √¢≈ì‚Äú No significant drift detected\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"√¢≈ì‚Äî Error processing table {table_name}: {e}\", exc_info=True)\n",
    "        print(f\"\\n√¢≈ì‚Äî Error processing {table_name}: {e}\")\n",
    "        results_summary.append({\n",
    "            'table_name': table_name,\n",
    "            'error': str(e)\n",
    "        })\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"PySpark drift detection completed for all tables\")\n",
    "logger.info(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a4ee402-2ba4-4198-8128-50b10dee9454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Detailed Summary Report\n",
    "\n",
    "Display comprehensive drift detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b229464c-8b65-4830-b5d6-4dc2efe22553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DRIFT DETECTION SUMMARY (PySpark Version)\n",
      "================================================================================\n",
      "\n",
      "√¢≈ì‚Äú Successfully processed 3 tables using PySpark\n",
      "\n",
      "        Table  Total Cols  Numerical  Categorical  Drifted Drift %   Status\n",
      "customer_data           9          4            5        6  75.00% √¢≈° √Ø¬∏¬è DRIFT\n",
      "product_sales           4          2            2        3  75.00% √¢≈° √Ø¬∏¬è DRIFT\n",
      "user_behavior           9          3            6        5  62.50% √¢≈° √Ø¬∏¬è DRIFT\n",
      "\n",
      "================================================================================\n",
      "Drifted Column Details:\n",
      "================================================================================\n",
      "\n",
      "customer_data:\n",
      "  Drifted columns: income, age, credit_score, data_version, account_type, region\n",
      "\n",
      "product_sales:\n",
      "  Drifted columns: revenue, price, product_category\n",
      "\n",
      "user_behavior:\n",
      "  Drifted columns: session_duration_minutes, pages_viewed, device_type, data_version, traffic_source\n",
      "\n",
      "================================================================================\n",
      "Overall Statistics:\n",
      "  - Total tables processed: 3\n",
      "  - Tables with drift: 3\n",
      "  - Tables without drift: 0\n",
      "  - Total columns analyzed: 22\n",
      "  - Total drifted columns: 14\n",
      "  - Overall drift rate: 63.64%\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(results_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DRIFT DETECTION SUMMARY (PySpark Version)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'error' in summary_df.columns:\n",
    "    # Show tables with errors\n",
    "    error_tables = summary_df[summary_df['error'].notna()]\n",
    "    if not error_tables.empty:\n",
    "        print(\"\\n√¢≈° √Ø¬∏¬è  Tables with errors:\")\n",
    "        for _, row in error_tables.iterrows():\n",
    "            print(f\"  - {row['table_name']}: {row['error']}\")\n",
    "        print()\n",
    "else:\n",
    "    # Ensure 'error' column exists (add it if missing)\n",
    "    summary_df['error'] = None  # or pd.NA\n",
    "    \n",
    "# Show tables without errors\n",
    "success_df = summary_df[~summary_df['table_name'].isin(summary_df[summary_df.get('error', pd.Series()).notna()]['table_name'])]\n",
    "\n",
    "if not success_df.empty:\n",
    "    print(f\"\\n√¢≈ì‚Äú Successfully processed {len(success_df)} tables using PySpark\\n\")\n",
    "    \n",
    "    # Display summary table\n",
    "    display_columns = ['table_name', 'total_columns', 'numerical_columns', \n",
    "                      'categorical_columns', 'drifted_columns', 'drift_share', 'dataset_drift']\n",
    "    display_df = success_df[display_columns].copy()\n",
    "    display_df['drift_share'] = display_df['drift_share'].apply(lambda x: f\"{x:.2%}\")\n",
    "    display_df['drift_status'] = display_df['dataset_drift'].apply(lambda x: '√¢≈° √Ø¬∏¬è DRIFT' if x else '√¢≈ì‚Äú OK')\n",
    "    display_df = display_df.drop('dataset_drift', axis=1)\n",
    "    display_df.columns = ['Table', 'Total Cols', 'Numerical', 'Categorical', \n",
    "                         'Drifted', 'Drift %', 'Status']\n",
    "    \n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    # Show drifted columns for tables with drift\n",
    "    drifted_tables = success_df[success_df['dataset_drift'] == True]\n",
    "    if not drifted_tables.empty:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"Drifted Column Details:\")\n",
    "        print(f\"{'='*80}\")\n",
    "        for _, row in drifted_tables.iterrows():\n",
    "            print(f\"\\n{row['table_name']}:\")\n",
    "            print(f\"  Drifted columns: {row['drifted_column_names']}\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_drifted = (display_df['Status'] == '√¢≈° √Ø¬∏¬è DRIFT').sum()\n",
    "    total_ok = (display_df['Status'] == '√¢≈ì‚Äú OK').sum()\n",
    "    total_cols = display_df['Total Cols'].sum()\n",
    "    total_drifted_cols = success_df['drifted_columns'].sum()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Overall Statistics:\")\n",
    "    print(f\"  - Total tables processed: {len(display_df)}\")\n",
    "    print(f\"  - Tables with drift: {total_drifted}\")\n",
    "    print(f\"  - Tables without drift: {total_ok}\")\n",
    "    print(f\"  - Total columns analyzed: {total_cols}\")\n",
    "    print(f\"  - Total drifted columns: {total_drifted_cols}\")\n",
    "    if total_cols > 0:\n",
    "        print(f\"  - Overall drift rate: {(total_drifted_cols/total_cols)*100:.2f}%\")\n",
    "    print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b688f48a-91b7-42f7-ac7f-e374ddca0a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Access Reports\n",
    "\n",
    "Information on accessing generated reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bd8f3cd-a4cb-48d2-a0fe-31d98becb064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Report Locations:\n",
      "================================================================================\n",
      "\n",
      "üìÅ Local Reports:\n",
      "   Path: reports\n",
      "   - HTML reports: reports/html/\n",
      "   - JSON reports: reports/json/\n",
      "\n",
      "================================================================================\n",
      "\n",
      "To view reports:\n",
      "  1. HTML Reports (local): Open in web browser for interactive visualization\n",
      "  2. JSON Reports (local): Use for programmatic analysis or monitoring integration\n",
      "  3. ADLS (if enabled): Access via abfss path in your storage account\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Report Locations:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Local reports are saved by default for testing\n",
    "print(f\"\\nüìÅ Local Reports:\")\n",
    "print(f\"   Path: reports\")\n",
    "print(f\"   - HTML reports: reports/html/\")\n",
    "print(f\"   - JSON reports: reports/json/\")\n",
    "\n",
    "if config.is_adls_output_enabled():\n",
    "    adls_config = config.get_adls_config()\n",
    "    print(f\"\\n√¢Àú¬Å√Ø¬∏¬è  ADLS Reports:\")\n",
    "    print(f\"   Container: {adls_config['container']}\")\n",
    "    print(f\"   Base Path: {adls_config['base_path']}\")\n",
    "    print(f\"   Storage Account: {adls_config['storage_account']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nTo view reports:\")\n",
    "print(\"  1. HTML Reports (local): Open in web browser for interactive visualization\")\n",
    "print(\"  2. JSON Reports (local): Use for programmatic analysis or monitoring integration\")\n",
    "print(\"  3. ADLS (if enabled): Access via abfss path in your storage account\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66d4abd0-003e-475e-9166-f36d0f85540e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "### PySpark Advantages\n",
    "- **Scalability**: Handles large datasets efficiently using distributed processing\n",
    "- **Performance**: Leverages Spark's in-memory computation and optimization\n",
    "- **Integration**: Seamless integration with Databricks Unity Catalog\n",
    "- **Resource Management**: Better memory management for large-scale operations\n",
    "\n",
    "### Key Differences from Python Version\n",
    "1. **Data Loading**: Uses Spark for distributed data loading\n",
    "2. **Processing**: Leverages Spark's parallel processing capabilities\n",
    "3. **Memory**: More efficient memory usage for large datasets\n",
    "4. **Sampling**: Can apply distributed sampling before conversion to Pandas\n",
    "\n",
    "### Next Steps\n",
    "1. **Review Reports**: Analyze detailed drift reports for affected tables\n",
    "2. **Investigate Causes**: Identify root causes of detected drift\n",
    "3. **Tune Configuration**: Adjust thresholds and tests based on results\n",
    "4. **Schedule Jobs**: Set up Databricks jobs for automated monitoring\n",
    "5. **Set Alerts**: Configure alerting based on drift detection results\n",
    "6. **Scale Up**: Increase cluster size for larger datasets if needed\n",
    "\n",
    "### Production Tips\n",
    "- Use Databricks job scheduling for regular drift monitoring\n",
    "- Enable ADLS output for centralized report storage\n",
    "- Configure appropriate sampling for very large tables (>100M rows)\n",
    "- Use Delta Lake time travel for precise version comparison\n",
    "- Set up downstream alerts and notifications\n",
    "- Monitor Spark job performance and optimize as needed"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_drift_detection_pyspark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
