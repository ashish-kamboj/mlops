{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Real-Time Inference\n",
    "\n",
    "Complete workflow for serving MLflow models locally.\n",
    "\n",
    "**Steps:**\n",
    "1. Load config & discover latest run\n",
    "2. Load signature / feature columns artifact\n",
    "3. Prepare & align sample data\n",
    "4. Serve model locally (separate terminal)\n",
    "5. Health check & prediction via HTTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, logging\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import yaml\n",
    "import requests\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "from utils.common_utils import load_config, setup_logging\n",
    "from utils.data_loader import load_data_from_source\n",
    "\n",
    "config = load_config('../config/config.yaml')\n",
    "setup_logging(config)\n",
    "\n",
    "# Ensure tracking URI is set BEFORE creating a client\n",
    "mode = config.get('environment', {}).get('mode', 'local')\n",
    "if mode == 'databricks':\n",
    "    tracking_uri = config['mlflow']['databricks']['tracking_uri']\n",
    "else:\n",
    "    tracking_uri = config['mlflow']['local']['tracking_uri']\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "print(f\"Tracking URI set to: {tracking_uri}\")\n",
    "\n",
    "client = MlflowClient()\n",
    "print(\"MlflowClient initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Discover Latest Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_run_from_filesystem(mlruns_path='../mlruns', experiment_name=None):\n",
    "    \"\"\"Fallback: Scan filesystem directly when MLflow client can't see experiments\"\"\"\n",
    "    import os\n",
    "    import yaml\n",
    "    from pathlib import Path\n",
    "    \n",
    "    mlruns = Path(mlruns_path)\n",
    "    if not mlruns.exists():\n",
    "        return None\n",
    "    \n",
    "    latest_run = None\n",
    "    latest_time = 0\n",
    "    \n",
    "    # Scan all experiment directories\n",
    "    for exp_dir in mlruns.iterdir():\n",
    "        if not exp_dir.is_dir() or exp_dir.name in ['.trash', 'models']:\n",
    "            continue\n",
    "        \n",
    "        # Check experiment meta\n",
    "        meta_file = exp_dir / 'meta.yaml'\n",
    "        if not meta_file.exists():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with open(meta_file, 'r') as f:\n",
    "                exp_meta = yaml.safe_load(f)\n",
    "            \n",
    "            # Filter by experiment name if specified\n",
    "            if experiment_name and exp_meta.get('name') != experiment_name:\n",
    "                continue\n",
    "            \n",
    "            # Scan runs in this experiment\n",
    "            for run_dir in exp_dir.iterdir():\n",
    "                if not run_dir.is_dir() or run_dir.name in ['models', 'tags']:\n",
    "                    continue\n",
    "                \n",
    "                run_meta_file = run_dir / 'meta.yaml'\n",
    "                if not run_meta_file.exists():\n",
    "                    continue\n",
    "                \n",
    "                with open(run_meta_file, 'r') as f:\n",
    "                    run_meta = yaml.safe_load(f)\n",
    "                \n",
    "                start_time = run_meta.get('start_time', 0)\n",
    "                if start_time > latest_time:\n",
    "                    latest_time = start_time\n",
    "                    latest_run = {\n",
    "                        'run_id': run_meta['run_id'],\n",
    "                        'experiment_id': run_meta['experiment_id'],\n",
    "                        'start_time': start_time,\n",
    "                        'status': run_meta.get('status')\n",
    "                    }\n",
    "        except Exception as e:\n",
    "            logging.debug(f\"Error scanning {exp_dir}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return latest_run\n",
    "\n",
    "mode = config.get('environment', {}).get('mode', 'local')\n",
    "if mode == 'databricks':\n",
    "    experiment_name = config['mlflow']['databricks']['experiment_name']\n",
    "else:\n",
    "    experiment_name = config['mlflow']['local']['experiment_name']\n",
    "print(f\"Environment mode: {mode}\")\n",
    "print(f\"Target experiment name: {experiment_name}\")\n",
    "\n",
    "# Try filesystem scan as fallback\n",
    "latest_run_info = get_latest_run_from_filesystem('../mlruns', experiment_name)\n",
    "if not latest_run_info:\n",
    "    raise RuntimeError(\n",
    "        f\"No runs found for experiment '{experiment_name}'. Retrain the model first.\"\n",
    "    )\n",
    "\n",
    "run_id = latest_run_info['run_id']\n",
    "print(f'✅ Latest run: {run_id}')\n",
    "print(f'   Experiment ID: {latest_run_info[\"experiment_id\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Signature & Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_signature_and_features(run_id, experiment_id, mlruns_path='../mlruns'):\n",
    "    \"\"\"Load signature and features directly from filesystem\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    signature_cols = None\n",
    "    feature_cols = None\n",
    "    \n",
    "    # Build artifact path\n",
    "    run_dir = Path(mlruns_path) / experiment_id / run_id / 'artifacts'\n",
    "    \n",
    "    # Load signature from MLmodel\n",
    "    try:\n",
    "        mlmodel_path = run_dir / 'model' / 'MLmodel'\n",
    "        if mlmodel_path.exists():\n",
    "            with open(mlmodel_path, 'r', encoding='utf-8') as f:\n",
    "                yml = yaml.safe_load(f)\n",
    "            sig_inputs = yml.get('signature', {}).get('inputs', [])\n",
    "            signature_cols = [i.get('name') for i in sig_inputs if i.get('name')]\n",
    "            if signature_cols:\n",
    "                print(f'✅ Loaded signature: {len(signature_cols)} columns')\n",
    "        else:\n",
    "            print(f'⚠️  MLmodel not found at {mlmodel_path}')\n",
    "    except Exception as e:\n",
    "        logging.warning(f'Signature load failed: {e}')\n",
    "    \n",
    "    # Load feature columns artifact\n",
    "    try:\n",
    "        feat_art = run_dir / 'feature_columns.json'\n",
    "        if feat_art.exists():\n",
    "            with open(feat_art, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            feature_cols = data.get('feature_columns')\n",
    "            if feature_cols:\n",
    "                print(f'✅ Loaded feature artifact: {len(feature_cols)} columns')\n",
    "        else:\n",
    "            print(f'⚠️  feature_columns.json not found at {feat_art}')\n",
    "    except Exception as e:\n",
    "        logging.warning(f'Feature load failed: {e}')\n",
    "    \n",
    "    return signature_cols, feature_cols\n",
    "\n",
    "signature_cols, feature_cols = load_signature_and_features(run_id, latest_run_info['experiment_id'])\n",
    "print(f'Signature: {len(signature_cols) if signature_cols else None}')\n",
    "print(f'Feature artifact: {len(feature_cols) if feature_cols else None}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['data_source']['type'] == 'unity_catalog':\n",
    "    from utils.common_utils import get_spark_session\n",
    "    spark = get_spark_session(config)\n",
    "    features_df = load_data_from_source(config, 'customer_features', spark)\n",
    "else:\n",
    "    features_df = pd.read_csv('../data/processed/customer_features.csv')\n",
    "\n",
    "sample = features_df.sample(1, random_state=42)\n",
    "customer_id = sample['CUSTOMERID'].iloc[0]\n",
    "X_raw = sample.drop(['CUSTOMERID','PARTYID'], axis=1, errors='ignore')\n",
    "\n",
    "cat_cols = X_raw.select_dtypes(include=['object','category']).columns.tolist()\n",
    "if cat_cols:\n",
    "    X_raw = pd.get_dummies(X_raw, columns=cat_cols, drop_first=True)\n",
    "\n",
    "print(f'Customer ID: {customer_id}')\n",
    "print(f'Raw columns: {len(X_raw.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Align Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = signature_cols or feature_cols\n",
    "assert expected, 'No schema available. Retrain model after patch.'\n",
    "\n",
    "def align(df, expected_cols):\n",
    "    for c in expected_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0\n",
    "    to_drop = [c for c in df.columns if c not in expected_cols]\n",
    "    if to_drop:\n",
    "        df = df.drop(columns=to_drop)\n",
    "    return df[expected_cols]\n",
    "\n",
    "X_aligned = align(X_raw.copy(), expected)\n",
    "print(f'Aligned shape: {X_aligned.shape}')\n",
    "missing = [c for c in expected if c not in X_aligned.columns]\n",
    "extra = [c for c in X_aligned.columns if c not in expected]\n",
    "print(f'Missing: {len(missing)} | Extra: {len(extra)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Serve Commands\n",
    "\n",
    "Copy and run in a **separate PowerShell terminal**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Run-based (recommended):')\n",
    "print(f'mlflow models serve -m \"runs:/{run_id}/model\" -p 5000 --env-manager=local')\n",
    "print()\n",
    "model_name = config['mlflow']['local']['registered_model_name']\n",
    "print(f'Registry-based:')\n",
    "print(f'mlflow models serve -m \"models:/{model_name}/Production\" -p 5000 --env-manager=local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Health Check & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = 'http://127.0.0.1:5000'\n",
    "\n",
    "def check_health(url=ENDPOINT):\n",
    "    try:\n",
    "        r = requests.get(url + '/health', timeout=5)\n",
    "        return r.status_code == 200\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "healthy = check_health()\n",
    "print(f'Health: {healthy}')\n",
    "\n",
    "if healthy:\n",
    "    payload = {'dataframe_split': {'columns': X_aligned.columns.tolist(), 'data': X_aligned.values.tolist()}}\n",
    "    r = requests.post(ENDPOINT + '/invocations', json=payload, timeout=30)\n",
    "    print(f'Status: {r.status_code}')\n",
    "    if r.status_code == 200:\n",
    "        result = r.json()\n",
    "        print(json.dumps(result, indent=2))\n",
    "        if 'predictions' in result:\n",
    "            print(f'Recommended Product: {result[\"predictions\"][0]}')\n",
    "    else:\n",
    "        print(r.text[:500])\n",
    "else:\n",
    "    print('Start server first')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyperpersonalization-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
