{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c42a7ceb-17f9-436a-b89a-2885685f076e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Required Packages"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for real-time inference\n",
    "%pip install databricks-sdk --quiet\n",
    "%pip install databricks-feature-engineering --quiet\n",
    "%pip install requests --quiet\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39858d97-e58c-43a2-8c39-bbff8a2702cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint: next-best-product-endpoint\nModel Name: datafabric_catalog.customer_hc_silver.next_best_product_model\n"
     ]
    }
   ],
   "source": [
    "import os, json, logging, pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "from utils.common_utils import load_config, setup_logging\n",
    "from utils.data_loader import load_data_from_source\n",
    "\n",
    "config = load_config('../config/config.yaml')\n",
    "setup_logging(config)\n",
    "endpoint_name = config['deployment']['endpoint_name']\n",
    "model_name = config['mlflow']['databricks']['registered_model_name']\n",
    "print('Endpoint:', endpoint_name)\n",
    "print('Model Name:', model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86d8f194-d6ed-4cb1-925e-e4bf48e7ade6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Prepare Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23e6e463-ec22-4707-abb8-36ba30943034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCA Loading features from Feature Store: datafabric_catalog.ml_outputs.customer_features_fs\n✅ Loaded features from Feature Store\n✅ Loaded features for 1000 customers\n\n⚠️ Found 4 categorical columns: ['AGE_GROUP', 'TENURE_GROUP', 'MIN_INTEREST_RATE', 'MAX_INTEREST_RATE']\n   These will be sent as-is to the endpoint\n\n\uD83D\uDC64 Sample Customer ID: 1522\n\uD83D\uDCCA Features: 41 columns\n\nFeature columns: ['AGE', 'CUSTOMER_TENURE_DAYS', 'AGE_GROUP', 'TENURE_GROUP', 'TOTAL_ACCOUNTS', 'ACTIVE_ACCOUNTS', 'TOTAL_PRINCIPAL', 'AVG_PRINCIPAL', 'MAX_PRINCIPAL', 'AVG_INTEREST_RATE']...\n"
     ]
    }
   ],
   "source": [
    "# Load customer features from Feature Store or Unity Catalog\n",
    "if config['data_source']['type'] == 'unity_catalog':\n",
    "    from utils.common_utils import get_spark_session\n",
    "    spark = get_spark_session(config)\n",
    "    \n",
    "    uc_config = config['data_source']['unity_catalog']\n",
    "    catalog = uc_config['catalog']\n",
    "    output_schema = uc_config['output_schema']\n",
    "    \n",
    "    feature_store_table = f\"{catalog}.{output_schema}.customer_features_fs\"\n",
    "    features_table = f\"{catalog}.{output_schema}.customer_features\"\n",
    "    \n",
    "    print(f\"\uD83D\uDCCA Loading features from Feature Store: {feature_store_table}\")\n",
    "    \n",
    "    # Try to load from Feature Store first\n",
    "    try:\n",
    "        from databricks.feature_engineering import FeatureEngineeringClient\n",
    "        \n",
    "        fe = FeatureEngineeringClient()\n",
    "        features_spark_df = fe.read_table(name=feature_store_table)\n",
    "        features_df = features_spark_df.toPandas()\n",
    "        print(f\"✅ Loaded features from Feature Store\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(f\"⚠️ Feature Engineering client not available, using direct table read\")\n",
    "        features_df = spark.table(features_table).toPandas()\n",
    "        print(f\"✅ Loaded features from Unity Catalog table\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading from Feature Store: {str(e)}\")\n",
    "        features_df = spark.table(features_table).toPandas()\n",
    "        print(f\"✅ Loaded features from Unity Catalog table\")\n",
    "else:\n",
    "    # CSV mode\n",
    "    features_path = os.path.abspath('../data/processed/customer_features.csv')\n",
    "    print(f\"\uD83D\uDCC2 Loading features from: {features_path}\")\n",
    "    features_df = pd.read_csv(features_path)\n",
    "\n",
    "print(f\"✅ Loaded features for {len(features_df)} customers\")\n",
    "\n",
    "# Select a sample customer for testing\n",
    "sample = features_df.sample(1, random_state=42)\n",
    "cust_id = int(sample['CUSTOMERID'].iloc[0])\n",
    "\n",
    "# Prepare features (drop ID columns)\n",
    "X_raw = sample.drop(['CUSTOMERID', 'PARTYID'], axis=1, errors='ignore')\n",
    "\n",
    "# Check for categorical columns\n",
    "cat_cols = X_raw.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "if cat_cols:\n",
    "    print(f\"\\n⚠️ Found {len(cat_cols)} categorical columns: {cat_cols}\")\n",
    "    print(f\"   These will be sent as-is to the endpoint\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDC64 Sample Customer ID: {cust_id}\")\n",
    "print(f\"\uD83D\uDCCA Features: {len(X_raw.columns)} columns\")\n",
    "print(f\"\\nFeature columns: {X_raw.columns.tolist()[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a893e95-664b-42df-b658-929c7844b0bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preprocess Features (One-Hot Encoding)"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "091e89a2-ad1b-4a01-b30e-c0917a33f95f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Build Payload (dataframe_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8403958-c51b-46b4-8719-eeb39dc9dfc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCA Original features: 41 columns\nColumns: ['AGE', 'CUSTOMER_TENURE_DAYS', 'AGE_GROUP', 'TENURE_GROUP', 'TOTAL_ACCOUNTS', 'ACTIVE_ACCOUNTS', 'TOTAL_PRINCIPAL', 'AVG_PRINCIPAL', 'MAX_PRINCIPAL', 'AVG_INTEREST_RATE', 'MIN_INTEREST_RATE', 'MAX_INTEREST_RATE', 'AVG_ACCOUNT_AGE_DAYS', 'MIN_ACCOUNT_AGE_DAYS', 'MAX_ACCOUNT_AGE_DAYS', 'NUM_UNIQUE_PRODUCTS', 'NUM_UNIQUE_CHANNELS', 'INACTIVE_ACCOUNTS', 'ACTIVE_ACCOUNT_RATIO', 'TXN_COUNT_SHORT_TERM', 'TXN_AMOUNT_SUM_SHORT_TERM', 'TXN_AMOUNT_MEAN_SHORT_TERM', 'TXN_AMOUNT_MEDIAN_SHORT_TERM', 'TXN_AMOUNT_STD_SHORT_TERM', 'TXN_AMOUNT_MIN_SHORT_TERM', 'TXN_AMOUNT_MAX_SHORT_TERM', 'TXN_COUNT_MEDIUM_TERM', 'TXN_AMOUNT_SUM_MEDIUM_TERM', 'TXN_AMOUNT_MEAN_MEDIUM_TERM', 'TXN_AMOUNT_MEDIAN_MEDIUM_TERM', 'TXN_AMOUNT_STD_MEDIUM_TERM', 'TXN_AMOUNT_MIN_MEDIUM_TERM', 'TXN_AMOUNT_MAX_MEDIUM_TERM', 'TXN_COUNT_LONG_TERM', 'TXN_AMOUNT_SUM_LONG_TERM', 'TXN_AMOUNT_MEAN_LONG_TERM', 'TXN_AMOUNT_MEDIAN_LONG_TERM', 'TXN_AMOUNT_STD_LONG_TERM', 'TXN_AMOUNT_MIN_LONG_TERM', 'TXN_AMOUNT_MAX_LONG_TERM', 'DAYS_SINCE_LAST_TXN']\n\n\uD83D\uDD04 Applying one-hot encoding to: ['AGE_GROUP', 'TENURE_GROUP']\n✅ Keeping MIN_INTEREST_RATE and MAX_INTEREST_RATE as numeric (not one-hot encoded)\n✅ One-hot encoding complete: 39 columns\n\n\uD83D\uDCCA Final feature count: 46 columns\nSample features: ['AGE', 'CUSTOMER_TENURE_DAYS', 'TOTAL_ACCOUNTS', 'ACTIVE_ACCOUNTS', 'TOTAL_PRINCIPAL', 'AVG_PRINCIPAL', 'MAX_PRINCIPAL', 'AVG_INTEREST_RATE', 'MIN_INTEREST_RATE', 'MAX_INTEREST_RATE', 'AVG_ACCOUNT_AGE_DAYS', 'MIN_ACCOUNT_AGE_DAYS', 'MAX_ACCOUNT_AGE_DAYS', 'NUM_UNIQUE_PRODUCTS', 'NUM_UNIQUE_CHANNELS']...\n\n\uD83D\uDCE6 Payload Preview:\n================================================================================\n{\n  \"inputs\": [\n    {\n      \"AGE\": 46,\n      \"CUSTOMER_TENURE_DAYS\": 170,\n      \"TOTAL_ACCOUNTS\": 1.0,\n      \"ACTIVE_ACCOUNTS\": 0.0,\n      \"TOTAL_PRINCIPAL\": 825000.0,\n      \"AVG_PRINCIPAL\": 825000.0,\n      \"MAX_PRINCIPAL\": 825000.0,\n      \"AVG_INTEREST_RATE\": 0.064,\n      \"MIN_INTEREST_RATE\": 0.064,\n      \"MAX_INTEREST_RATE\": 0.064,\n      \"AVG_ACCOUNT_AGE_DAYS\": 327.0,\n      \"MIN_ACCOUNT_AGE_DAYS\": 327.0,\n      \"MAX_ACCOUNT_AGE_DAYS\": 327.0,\n      \"NUM_UNIQUE_PRODUCTS\": 1.0,\n      \"NUM_UNIQUE_CHANNELS\": 1.0,\n      \"INACTIVE_ACCOUNTS\": 1.0,\n      \"ACTIVE_ACCOUNT_RATIO\": 0.0,\n      \"TXN_COUNT_SHORT_TERM\": 3.0,\n      \"TXN_AMOUNT_SUM_SHORT_TERM\": 7400.0,\n      \"TXN_AMOUNT_MEAN_SHORT_TERM\": 2466.6666666667,\n      \"TXN_AMOUNT_MEDIAN_SHORT_TERM\": 450.0,\n      \"TXN_AMOUNT_STD_SHORT_TERM\": 3492.96\n...\n================================================================================\n\n\uD83D\uDCCA Payload size: 1473 bytes\n\uD83D\uDCC4 Number of records: 1\n\uD83D\uDCCA Features per record: 46\n\n✅ Payload ready for NEW model (46 features with numeric interest rates)\n"
     ]
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert Decimal types to float for JSON serialization\n",
    "X_processed = X_raw.copy()\n",
    "for col in X_processed.columns:\n",
    "    if X_processed[col].dtype == 'object':\n",
    "        # Check if the column contains Decimal objects\n",
    "        if len(X_processed[col]) > 0 and isinstance(X_processed[col].iloc[0], Decimal):\n",
    "            X_processed[col] = X_processed[col].astype(float)\n",
    "\n",
    "print(f\"\uD83D\uDCCA Original features: {len(X_processed.columns)} columns\")\n",
    "print(f\"Columns: {X_processed.columns.tolist()}\")\n",
    "\n",
    "# Apply one-hot encoding ONLY to AGE_GROUP and TENURE_GROUP\n",
    "# Keep MIN_INTEREST_RATE and MAX_INTEREST_RATE as numeric (matching the NEW model)\n",
    "cat_cols_to_encode = ['AGE_GROUP', 'TENURE_GROUP']\n",
    "existing_cat_cols = [col for col in cat_cols_to_encode if col in X_processed.columns]\n",
    "\n",
    "if existing_cat_cols:\n",
    "    print(f\"\\n\uD83D\uDD04 Applying one-hot encoding to: {existing_cat_cols}\")\n",
    "    print(f\"✅ Keeping MIN_INTEREST_RATE and MAX_INTEREST_RATE as numeric (not one-hot encoded)\")\n",
    "    \n",
    "    # Apply one-hot encoding with drop_first=True to match training\n",
    "    X_processed = pd.get_dummies(X_processed, columns=existing_cat_cols, drop_first=True, dtype=int)\n",
    "    \n",
    "    print(f\"✅ One-hot encoding complete: {len(X_processed.columns)} columns\")\n",
    "\n",
    "# Add missing one-hot encoded columns with 0 values (for categories not in this sample)\n",
    "expected_age_groups = ['AGE_GROUP_26-35', 'AGE_GROUP_36-45', 'AGE_GROUP_46-55', 'AGE_GROUP_56-65', 'AGE_GROUP_65+']\n",
    "expected_tenure_groups = ['TENURE_GROUP_1-2Y', 'TENURE_GROUP_6M-1Y']\n",
    "\n",
    "for col in expected_age_groups + expected_tenure_groups:\n",
    "    if col not in X_processed.columns:\n",
    "        X_processed[col] = 0\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA Final feature count: {len(X_processed.columns)} columns\")\n",
    "print(f\"Sample features: {X_processed.columns.tolist()[:15]}...\")\n",
    "\n",
    "# Convert to JSON-serializable format\n",
    "records = json.loads(X_processed.to_json(orient='records'))\n",
    "\n",
    "# Build payload\n",
    "payload_to_send = {'inputs': records}\n",
    "\n",
    "print(\"\\n\uD83D\uDCE6 Payload Preview:\")\n",
    "print(\"=\" * 80)\n",
    "print(json.dumps(payload_to_send, indent=2)[:800])\n",
    "print(\"...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n\uD83D\uDCCA Payload size: {len(json.dumps(payload_to_send))} bytes\")\n",
    "print(f\"\uD83D\uDCC4 Number of records: {len(payload_to_send['inputs'])}\")\n",
    "print(f\"\uD83D\uDCCA Features per record: {len(payload_to_send['inputs'][0]) if payload_to_send['inputs'] else 0}\")\n",
    "\n",
    "print(\"\\n✅ Payload ready for NEW model (46 features with numeric interest rates)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6620f6c2-6035-4ed7-bd10-24fe50deef37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Invoke Databricks Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bda0fbd1-5cce-47f4-b5de-05346223a300",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check Endpoint Status"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc502d67-9770-450a-9d94-361c40495c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDE80 Invoking endpoint: next-best-product-endpoint\n\uD83D\uDC64 Customer ID: 1522\n\n\uD83D\uDD17 Endpoint URL: https://adb-1364099644588382.2.azuredatabricks.net/serving-endpoints/next-best-product-endpoint/invocations\n\n\uD83D\uDCE4 Sending request with 'inputs' format...\nPayload preview: {'inputs': [{'AGE': 46, 'CUSTOMER_TENURE_DAYS': 170, 'TOTAL_ACCOUNTS': 1.0, 'ACTIVE_ACCOUNTS': 0.0, 'TOTAL_PRINCIPAL': 825000.0, 'AVG_PRINCIPAL': 825000.0, 'MAX_PRINCIPAL': 825000.0, 'AVG_INTEREST_RAT...\n\n⏱️ Response time: 0.14 seconds\n\uD83D\uDCCA Status code: 200\n\n✅ Request successful!\n"
     ]
    }
   ],
   "source": [
    "# Invoke the serving endpoint\n",
    "import requests\n",
    "import time\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "print(f\"\\n\uD83D\uDE80 Invoking endpoint: {endpoint_name}\")\n",
    "print(f\"\uD83D\uDC64 Customer ID: {cust_id}\")\n",
    "\n",
    "try:\n",
    "    # Get workspace client for host information\n",
    "    w = WorkspaceClient()\n",
    "    \n",
    "    # Get host (remove https:// if present)\n",
    "    host = w.config.host\n",
    "    if host.startswith('https://'):\n",
    "        host = host.replace('https://', '')\n",
    "    \n",
    "    # Get endpoint URL\n",
    "    endpoint_url = f\"https://{host}/serving-endpoints/{endpoint_name}/invocations\"\n",
    "    \n",
    "    # Get authentication token from notebook context (works inside Databricks)\n",
    "    try:\n",
    "        token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "    except Exception:\n",
    "        # Fallback to environment variable if not in notebook context\n",
    "        token = os.environ.get('DATABRICKS_TOKEN')\n",
    "        if not token:\n",
    "            raise ValueError(\"No authentication token available. Set DATABRICKS_TOKEN environment variable.\")\n",
    "    \n",
    "    print(f\"\\n\uD83D\uDD17 Endpoint URL: {endpoint_url}\")\n",
    "    \n",
    "    # Try alternative payload format (inputs)\n",
    "    payload_to_send = {\n",
    "        'inputs': X_processed.to_dict(orient='records')\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n\uD83D\uDCE4 Sending request with 'inputs' format...\")\n",
    "    print(f\"Payload preview: {str(payload_to_send)[:200]}...\")\n",
    "    \n",
    "    # Set headers\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {token}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    # Send request\n",
    "    start_time = time.time()\n",
    "    resp = requests.post(endpoint_url, json=payload_to_send, headers=headers, timeout=60)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n⏱️ Response time: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"\uD83D\uDCCA Status code: {resp.status_code}\")\n",
    "    \n",
    "    if resp.status_code == 200:\n",
    "        print(f\"\\n✅ Request successful!\")\n",
    "        response_data = resp.json()\n",
    "    else:\n",
    "        print(f\"\\n❌ Request failed!\")\n",
    "        print(f\"\\nFull Response:\")\n",
    "        print(resp.text)  # Print full error message\n",
    "        response_data = None\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error invoking endpoint: {str(e)}\")\n",
    "    response_data = None\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f59d3b1f-0374-4e7f-8107-7c58cf32bdd0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fixed prepare_training_data Function (Copy to utils/model_training.py)"
    }
   },
   "source": [
    "## 4. Response Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d12d6284-4b9d-436e-a225-9ae03c4a5d90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nPREDICTION RESULTS\n================================================================================\n\n\uD83D\uDC64 Customer ID: 1522\n\n\uD83C\uDFAF Predicted Product ID: 6\n\n================================================================================\n✅ Real-time inference completed successfully!\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Parse and display predictions\n",
    "if response_data:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PREDICTION RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\n\uD83D\uDC64 Customer ID: {cust_id}\")\n",
    "    \n",
    "    # The response format depends on the model output\n",
    "    # Typically it's {'predictions': [...]}\n",
    "    if 'predictions' in response_data:\n",
    "        predictions = response_data['predictions']\n",
    "        \n",
    "        if isinstance(predictions, list) and len(predictions) > 0:\n",
    "            pred = predictions[0]\n",
    "            \n",
    "            # If prediction is a list (multi-class probabilities)\n",
    "            if isinstance(pred, list):\n",
    "                print(f\"\\n\uD83C\uDFAF Top 5 Product Recommendations:\")\n",
    "                print(\"\\n{:<6} {:<15} {:<12}\".format(\"Rank\", \"Product ID\", \"Probability\"))\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                # Get top 5 predictions\n",
    "                top_5_indices = sorted(range(len(pred)), key=lambda i: pred[i], reverse=True)[:5]\n",
    "                \n",
    "                for rank, idx in enumerate(top_5_indices, 1):\n",
    "                    prob = pred[idx]\n",
    "                    print(\"{:<6} {:<15} {:<12.4f}\".format(rank, idx, prob))\n",
    "            \n",
    "            # If prediction is a single value\n",
    "            else:\n",
    "                print(f\"\\n\uD83C\uDFAF Predicted Product ID: {pred}\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"\\n\uD83D\uDCCA Raw predictions: {predictions}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"\\n\uD83D\uDCCA Full response:\")\n",
    "        print(json.dumps(response_data, indent=2)[:1000])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✅ Real-time inference completed successfully!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n❌ No response data available. Check the error above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "586e9978-730d-46b5-a5b2-8edf3bfc2f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Best Practices\n",
    "\n",
    "- Promote model to Production stage before creating endpoint.\n",
    "- Use staged rollout (shadow traffic) for new versions.\n",
    "- Monitor latency, error rate, drift.\n",
    "- Log request IDs for traceability.\n",
    "- Add input validation & authentication."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07_realtime_inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}