{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "import os\n",
        "import logging\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Add utils to path\n",
        "sys.path.append(os.path.abspath('../'))\n",
        "\n",
        "from utils.common_utils import (\n",
        "    load_config, setup_logging, get_spark_session,\n",
        "    print_section_header, Timer\n",
        ")\n",
        "from utils.data_loader import save_data_to_destination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "              Data Generation for Next Best Product Recommendation              \n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load configuration\n",
        "config = load_config('../config/config.yaml')\n",
        "setup_logging(config)\n",
        "\n",
        "print_section_header(\"Data Generation for Next Best Product Recommendation\")\n",
        "logging.info(\"Starting data generation process...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Generate Synthetic Data\n",
        "# MAGIC\n",
        "We'll create realistic synthetic data for demonstration purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_address_data(n_addresses=1000):\n",
        "    \"\"\"Generate synthetic address data.\"\"\"\n",
        "    cities = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', \n",
        "              'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', 'San Jose']\n",
        "    states = ['NY', 'CA', 'IL', 'TX', 'AZ', 'PA', 'TX', 'CA', 'TX', 'CA']\n",
        "    \n",
        "    np.random.seed(config['random_seed'])\n",
        "    \n",
        "    addresses = pd.DataFrame({\n",
        "        'ADDRESSID': range(1, n_addresses + 1),\n",
        "        'STREETLINE1': [f\"{np.random.randint(1, 9999)} {np.random.choice(['Main', 'Oak', 'Park', 'Elm', 'Washington'])} St\"\n",
        "                        for _ in range(n_addresses)],\n",
        "        'CITY': np.random.choice(cities, n_addresses),\n",
        "        'STATE': [states[cities.index(city)] if city in cities else 'CA'\n",
        "                  for city in np.random.choice(cities, n_addresses)],\n",
        "        'POSTALCODE': [f\"{np.random.randint(10000, 99999)}\" for _ in range(n_addresses)],\n",
        "        'COUNTRY': ['USA'] * n_addresses\n",
        "    })\n",
        "    \n",
        "    logging.info(f\"Generated {len(addresses)} address records\")\n",
        "    return addresses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_banking_product_data():\n",
        "    \"\"\"Generate banking product data.\"\"\"\n",
        "    products = pd.DataFrame({\n",
        "        'PRODUCTID': [1, 2, 3, 4, 5, 6, 7],\n",
        "        'PRODUCTNAME': ['Personal Loan', 'Home Loan', 'Credit Card', 'Auto Loan', \n",
        "                       'Business Loan', 'Student Loan', 'Savings Account'],\n",
        "        'PRODUCTDESCRIPTION': [\n",
        "            'Personal loan for various expenses',\n",
        "            'Mortgage loan for home purchase',\n",
        "            'Credit card with rewards',\n",
        "            'Vehicle financing loan',\n",
        "            'Small business lending',\n",
        "            'Education financing',\n",
        "            'High-yield savings account'\n",
        "        ],\n",
        "        'PRODUCTMINIMUMAMOUNT': [1000, 50000, 500, 5000, 10000, 2000, 100],\n",
        "        'PRODUCTMAXIMUMAMOUNT': [50000, 500000, 50000, 100000, 500000, 100000, 1000000],\n",
        "        'PRODUCTMINIMUMTERM': [6, 120, 0, 12, 12, 24, 0],\n",
        "        'PRODUCTMAXIMUMTERM': [60, 360, 0, 84, 120, 120, 0]\n",
        "    })\n",
        "    \n",
        "    logging.info(f\"Generated {len(products)} product records\")\n",
        "    return products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_channel_data():\n",
        "    \"\"\"Generate channel data.\"\"\"\n",
        "    channels = pd.DataFrame({\n",
        "        'CHANNELID': [1, 2, 3, 4, 5],\n",
        "        'CHANNELNAME': ['Online', 'Branch', 'Mobile App', 'Phone', 'Agent'],\n",
        "        'CHANNELTYPEID': [1, 2, 3, 4, 5]\n",
        "    })\n",
        "    \n",
        "    logging.info(f\"Generated {len(channels)} channel records\")\n",
        "    return channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_party_data(n_parties=5000, addresses_df=None):\n",
        "    \"\"\"Generate party (person) data.\"\"\"\n",
        "    np.random.seed(config['random_seed'])\n",
        "    \n",
        "    first_names = ['John', 'Jane', 'Michael', 'Emily', 'David', 'Sarah', 'Robert', 'Lisa']\n",
        "    last_names = ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis']\n",
        "    \n",
        "    parties = pd.DataFrame({\n",
        "        'PARTYID': range(1, n_parties + 1),\n",
        "        'PARTYTYPE': ['PERSON'] * n_parties,\n",
        "        'LEGALNAME': [f\"{np.random.choice(first_names)} {np.random.choice(last_names)}\"\n",
        "                      for _ in range(n_parties)],\n",
        "        'DATEOFBIRTH': [datetime.now() - timedelta(days=np.random.randint(18*365, 70*365))\n",
        "                       for _ in range(n_parties)],\n",
        "        'PRIMARYADDRESSID': np.random.choice(addresses_df['ADDRESSID'].values, n_parties),\n",
        "        'PRIMARYEMAIL': [f\"customer{i}@email.com\" for i in range(1, n_parties + 1)],\n",
        "        'PRIMARYPHONE': [f\"{np.random.randint(2000000000, 9999999999, dtype=np.int64)}\" for _ in range(n_parties)]\n",
        "    })\n",
        "    \n",
        "    logging.info(f\"Generated {len(parties)} party records\")\n",
        "    return parties\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_customer_data(n_customers=5000, parties_df=None):\n",
        "    \"\"\"Generate customer data.\"\"\"\n",
        "    np.random.seed(config['random_seed'])\n",
        "    \n",
        "    customers = pd.DataFrame({\n",
        "        'CUSTOMERID': range(1, n_customers + 1),\n",
        "        'PARTYID': parties_df['PARTYID'].values[:n_customers],\n",
        "        'CUSTOMERESTABLISHEDDATE': [datetime.now() - timedelta(days=np.random.randint(1, 1825))\n",
        "                                   for _ in range(n_customers)],\n",
        "        'CUSTOMERTYPEID': np.random.choice([1, 2, 3], n_customers, p=[0.7, 0.2, 0.1])\n",
        "    })\n",
        "    \n",
        "    logging.info(f\"Generated {len(customers)} customer records\")\n",
        "    return customers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_customer_account_data(n_accounts=8000, customers_df=None, products_df=None, channels_df=None):\n",
        "    \"\"\"Generate customer account data with controlled future purchases for target creation.\n",
        "    \n",
        "    Strategy:\n",
        "      - Create historical accounts (origination before reference_date)\n",
        "      - Force a configurable proportion of customers (training_data_ratio) to have exactly 1 new account\n",
        "        originated AFTER the reference_date within the prediction window (default 90 days) so they\n",
        "        become labeled (NEXT_PRODUCT_ID available)\n",
        "    \n",
        "    This guarantees we can reach the desired labeled vs unlabeled customer ratio for training.\n",
        "    \"\"\"\n",
        "    np.random.seed(config['random_seed'])\n",
        "    \n",
        "    reference_date = datetime.strptime(config['feature_engineering']['reference_date'], '%Y-%m-%d')\n",
        "    prediction_window_days = 90  # keep aligned with feature engineering notebook usage\n",
        "    training_ratio = config['feature_engineering'].get('training_data_ratio', 0.80)\n",
        "    \n",
        "    total_customers = customers_df['CUSTOMERID'].nunique()\n",
        "    target_labeled_customers = int(total_customers * training_ratio)\n",
        "    \n",
        "    # Sample customers who will get a future (post-reference) account\n",
        "    labeled_customer_ids = set(np.random.choice(customers_df['CUSTOMERID'].values, size=target_labeled_customers, replace=False))\n",
        "    \n",
        "    accounts = []\n",
        "    \n",
        "    # We reserve one future account per labeled customer. Remaining accounts are historical.\n",
        "    historical_accounts_needed = n_accounts - len(labeled_customer_ids)\n",
        "    if historical_accounts_needed < 0:\n",
        "        # If n_accounts is too small, adjust so we at least create one account per labeled customer\n",
        "        logging.warning(\"n_accounts smaller than labeled customers requirement; increasing n_accounts\")\n",
        "        historical_accounts_needed = 0\n",
        "    \n",
        "    # --- Generate historical accounts (before reference_date) ---\n",
        "    for account_id in range(1, historical_accounts_needed + 1):\n",
        "        customer_id = np.random.choice(customers_df['CUSTOMERID'].values)\n",
        "        product_id = np.random.choice(products_df['PRODUCTID'].values)\n",
        "        channel_id = np.random.choice(channels_df['CHANNELID'].values)\n",
        "        \n",
        "        origination_date = reference_date - timedelta(days=np.random.randint(1, 730))  # up to 2 years back\n",
        "        product_info = products_df[products_df['PRODUCTID'] == product_id].iloc[0]\n",
        "        principal = np.random.uniform(product_info['PRODUCTMINIMUMAMOUNT'], product_info['PRODUCTMAXIMUMAMOUNT'])\n",
        "        maturity_days = np.random.randint(\n",
        "            product_info['PRODUCTMINIMUMTERM'] * 30,\n",
        "            max(product_info['PRODUCTMAXIMUMTERM'] * 30, 30)\n",
        "        ) if product_info['PRODUCTMAXIMUMTERM'] > 0 else 0\n",
        "        maturity_date = origination_date + timedelta(days=maturity_days) if maturity_days > 0 else None\n",
        "        \n",
        "        accounts.append({\n",
        "            'CUSTOMERACCOUNTID': account_id,\n",
        "            'CUSTOMERID': customer_id,\n",
        "            'PRODUCTID': product_id,\n",
        "            'CHANNELID': channel_id,\n",
        "            'ORIGINATIONDATE': origination_date,\n",
        "            'MATURITYDATE': maturity_date,\n",
        "            'PRINCIPALAMOUNT': round(principal, 2),\n",
        "            'INTERESTRATE': round(np.random.uniform(0.03, 0.15), 4),\n",
        "            'ACCOUNTSTATUS': np.random.choice(['ACTIVE', 'CLOSED', 'PENDING'], p=[0.7, 0.2, 0.1])\n",
        "        })\n",
        "    \n",
        "    # --- Generate future accounts (post reference) to create labels ---\n",
        "    next_account_id = historical_accounts_needed + 1\n",
        "    for customer_id in labeled_customer_ids:\n",
        "        product_id = np.random.choice(products_df['PRODUCTID'].values)\n",
        "        channel_id = np.random.choice(channels_df['CHANNELID'].values)\n",
        "        # Origination within prediction window\n",
        "        origination_date = reference_date + timedelta(days=np.random.randint(1, prediction_window_days + 1))\n",
        "        product_info = products_df[products_df['PRODUCTID'] == product_id].iloc[0]\n",
        "        principal = np.random.uniform(product_info['PRODUCTMINIMUMAMOUNT'], product_info['PRODUCTMAXIMUMAMOUNT'])\n",
        "        maturity_days = np.random.randint(\n",
        "            product_info['PRODUCTMINIMUMTERM'] * 30,\n",
        "            max(product_info['PRODUCTMAXIMUMTERM'] * 30, 30)\n",
        "        ) if product_info['PRODUCTMAXIMUMTERM'] > 0 else 0\n",
        "        maturity_date = origination_date + timedelta(days=maturity_days) if maturity_days > 0 else None\n",
        "        \n",
        "        accounts.append({\n",
        "            'CUSTOMERACCOUNTID': next_account_id,\n",
        "            'CUSTOMERID': customer_id,\n",
        "            'PRODUCTID': product_id,\n",
        "            'CHANNELID': channel_id,\n",
        "            'ORIGINATIONDATE': origination_date,\n",
        "            'MATURITYDATE': maturity_date,\n",
        "            'PRINCIPALAMOUNT': round(principal, 2),\n",
        "            'INTERESTRATE': round(np.random.uniform(0.03, 0.15), 4),\n",
        "            'ACCOUNTSTATUS': np.random.choice(['ACTIVE', 'CLOSED', 'PENDING'], p=[0.7, 0.2, 0.1])\n",
        "        })\n",
        "        next_account_id += 1\n",
        "    \n",
        "    accounts_df = pd.DataFrame(accounts)\n",
        "    \n",
        "    # Logging diagnostics\n",
        "    labeled_in_accounts = accounts_df[(accounts_df['ORIGINATIONDATE'] >= reference_date) & (accounts_df['ORIGINATIONDATE'] < reference_date + timedelta(days=prediction_window_days))]['CUSTOMERID'].nunique()\n",
        "    logging.info(f\"Generated {len(accounts_df)} customer account records\")\n",
        "    logging.info(f\"Forced labeled customers (future accounts within {prediction_window_days}d): {labeled_in_accounts}/{total_customers} ({labeled_in_accounts/total_customers*100:.1f}%)\")\n",
        "    logging.info(f\"Target labeled ratio: {training_ratio*100:.1f}%\")\n",
        "    \n",
        "    return accounts_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_transaction_data(n_transactions=50000, accounts_df=None):\n",
        "    \"\"\"Generate transaction data with reference to config reference_date.\n",
        "    \n",
        "    Generates transactions across a 2-year window:\n",
        "    - 50% before reference_date (for historical features)\n",
        "    - 50% after reference_date (for target variable - next product purchases)\n",
        "    \"\"\"\n",
        "    np.random.seed(config['random_seed'])\n",
        "    \n",
        "    # Get reference date from config\n",
        "    reference_date = datetime.strptime(config['feature_engineering']['reference_date'], '%Y-%m-%d')\n",
        "    \n",
        "    transactions = []\n",
        "    \n",
        "    # Split transactions: 50% before reference, 50% after\n",
        "    n_before = n_transactions // 2\n",
        "    n_after = n_transactions - n_before\n",
        "    \n",
        "    # Generate transactions BEFORE reference_date (for historical features)\n",
        "    for txn_id in range(1, n_before + 1):\n",
        "        account_id = np.random.choice(accounts_df['CUSTOMERACCOUNTID'].values)\n",
        "        # 730 days (2 years) before reference date\n",
        "        days_before = np.random.randint(1, 730)\n",
        "        txn_date = reference_date - timedelta(days=days_before)\n",
        "        \n",
        "        transactions.append({\n",
        "            'TRANSACTIONID': txn_id,\n",
        "            'CUSTOMERACCOUNTID': account_id,\n",
        "            'TRANSACTIONINITIATEDTIMESTAMP': txn_date,\n",
        "            'TRANSACTIONAMOUNT': round(np.random.uniform(10, 5000), 2),\n",
        "            'TRANSACTIONTYPEID': np.random.choice([1, 2, 3, 4])\n",
        "        })\n",
        "    \n",
        "    # Generate transactions AFTER reference_date (for target variable)\n",
        "    for txn_id in range(n_before + 1, n_transactions + 1):\n",
        "        account_id = np.random.choice(accounts_df['CUSTOMERACCOUNTID'].values)\n",
        "        # 180 days (6 months) after reference date to ensure coverage within prediction window\n",
        "        days_after = np.random.randint(1, 180)\n",
        "        txn_date = reference_date + timedelta(days=days_after)\n",
        "        \n",
        "        transactions.append({\n",
        "            'TRANSACTIONID': txn_id,\n",
        "            'CUSTOMERACCOUNTID': account_id,\n",
        "            'TRANSACTIONINITIATEDTIMESTAMP': txn_date,\n",
        "            'TRANSACTIONAMOUNT': round(np.random.uniform(10, 5000), 2),\n",
        "            'TRANSACTIONTYPEID': np.random.choice([1, 2, 3, 4])\n",
        "        })\n",
        "    \n",
        "    transactions_df = pd.DataFrame(transactions)\n",
        "    \n",
        "    # Log distribution for verification\n",
        "    before_ref = len(transactions_df[transactions_df['TRANSACTIONINITIATEDTIMESTAMP'] < reference_date])\n",
        "    after_ref = len(transactions_df[transactions_df['TRANSACTIONINITIATEDTIMESTAMP'] >= reference_date])\n",
        "    logging.info(f\"Generated {len(transactions_df)} transaction records\")\n",
        "    logging.info(f\"  - {before_ref} transactions before reference_date ({reference_date.date()})\")\n",
        "    logging.info(f\"  - {after_ref} transactions after reference_date\")\n",
        "    \n",
        "    return transactions_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_communication_data(n_communications=10000, parties_df=None):\n",
        "    \"\"\"Generate communication data.\"\"\"\n",
        "    np.random.seed(config['random_seed'])\n",
        "    \n",
        "    communications = []\n",
        "    \n",
        "    for comm_id in range(1, n_communications + 1):\n",
        "        party_id = np.random.choice(parties_df['PARTYID'].values)\n",
        "        comm_date = datetime.now() - timedelta(days=np.random.randint(1, 365))\n",
        "        \n",
        "        communications.append({\n",
        "            'COMMUNICATIONID': comm_id,\n",
        "            'PARTYID': party_id,\n",
        "            'COMMUNICATIONSTARTTIMESTAMP': comm_date,\n",
        "            'COMMUNICATIONDESCRIPTION': f\"Communication regarding account services\",\n",
        "            'COMMUNICATIONMETHODID': np.random.choice([1, 2, 3]),  # Email, Phone, SMS\n",
        "            'INBOUNDOUTBOUNDCOMMUNICATIONINDICATOR': np.random.choice([0, 1])\n",
        "        })\n",
        "    \n",
        "    communications_df = pd.DataFrame(communications)\n",
        "    logging.info(f\"Generated {len(communications_df)} communication records\")\n",
        "    return communications_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_document_data(n_documents=15000, parties_df=None, accounts_df=None):\n",
        "    \"\"\"Generate document data.\"\"\"\n",
        "    np.random.seed(config['random_seed'])\n",
        "    \n",
        "    documents = []\n",
        "    \n",
        "    for doc_id in range(1, n_documents + 1):\n",
        "        party_id = np.random.choice(parties_df['PARTYID'].values)\n",
        "        account_id = np.random.choice(accounts_df['CUSTOMERACCOUNTID'].values) if np.random.random() > 0.3 else None\n",
        "        doc_date = datetime.now() - timedelta(days=np.random.randint(1, 730))\n",
        "        \n",
        "        documents.append({\n",
        "            'DOCUMENTID': doc_id,\n",
        "            'PARTYID': party_id,\n",
        "            'CUSTOMERACCOUNTID': account_id,\n",
        "            'DOCUMENTNAME': np.random.choice(['ID Proof', 'Address Proof', 'Income Proof', 'Loan Agreement']),\n",
        "            'DOCUMENTTYPEID': np.random.choice([1, 2, 3, 4]),\n",
        "            'DOCUMENTCREATEDTIMESTAMP': doc_date\n",
        "        })\n",
        "    \n",
        "    documents_df = pd.DataFrame(documents)\n",
        "    logging.info(f\"Generated {len(documents_df)} document records\")\n",
        "    return documents_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Generate All Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Data generation completed successfully!\n"
          ]
        }
      ],
      "source": [
        "with Timer(\"Data Generation\"):\n",
        "    # Generate data in proper order (respecting foreign keys)\n",
        "    addresses = generate_address_data(n_addresses=1000)\n",
        "    products = generate_banking_product_data()\n",
        "    channels = generate_channel_data()\n",
        "    parties = generate_party_data(n_parties=5000, addresses_df=addresses)\n",
        "    customers = generate_customer_data(n_customers=5000, parties_df=parties)\n",
        "    accounts = generate_customer_account_data(n_accounts=8000, customers_df=customers, \n",
        "                                              products_df=products, channels_df=channels)\n",
        "    transactions = generate_transaction_data(n_transactions=50000, accounts_df=accounts)\n",
        "    communications = generate_communication_data(n_communications=10000, parties_df=parties)\n",
        "    documents = generate_document_data(n_documents=15000, parties_df=parties, accounts_df=accounts)\n",
        "    \n",
        "    print(\"\\nâœ… Data generation completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Save Generated Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data source type: csv\n",
            "Config raw path: ./data/raw\n",
            "Config output path: ./data/processed\n",
            "\n",
            "Actual save location (absolute path): d:\\home-credit-hyperpersonalization-poc\\data\\raw\n",
            "Files will be saved to project root data/raw folder âœ…\n"
          ]
        }
      ],
      "source": [
        "# Debug: Check configuration\n",
        "print(\"Data source type:\", config['data_source']['type'])\n",
        "print(\"Config raw path:\", config['data_source']['csv']['input_path'])\n",
        "print(\"Config output path:\", config['data_source']['csv']['output_path'])\n",
        "print(\"\\nActual save location (absolute path):\", os.path.abspath('../data/raw'))\n",
        "print(\"Files will be saved to project root data/raw folder âœ…\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Saved address: 1000 records\n",
            "âœ… Saved banking_product: 7 records\n",
            "âœ… Saved channel: 5 records\n",
            "âœ… Saved party: 5000 records\n",
            "âœ… Saved customer: 5000 records\n",
            "âœ… Saved customer_account: 8000 records\n",
            "âœ… Saved transaction: 50000 records\n",
            "âœ… Saved communication: 10000 records\n",
            "âœ… Saved document: 15000 records\n",
            "\n",
            "âœ… All data saved successfully!\n",
            "ðŸ“‚ Files saved to: d:\\home-credit-hyperpersonalization-poc\\data\\raw\n",
            "âœ… Saved transaction: 50000 records\n",
            "âœ… Saved communication: 10000 records\n",
            "âœ… Saved document: 15000 records\n",
            "\n",
            "âœ… All data saved successfully!\n",
            "ðŸ“‚ Files saved to: d:\\home-credit-hyperpersonalization-poc\\data\\raw\n"
          ]
        }
      ],
      "source": [
        "# Get Spark session (if using Unity Catalog)\n",
        "spark = None\n",
        "if config['data_source']['type'] == 'unity_catalog':\n",
        "    spark = get_spark_session(config)\n",
        "\n",
        "# Save all tables\n",
        "tables_dict = {\n",
        "    'address': addresses,\n",
        "    'banking_product': products,\n",
        "    'channel': channels,\n",
        "    'party': parties,\n",
        "    'customer': customers,\n",
        "    'customer_account': accounts,\n",
        "    'transaction': transactions,\n",
        "    'communication': communications,\n",
        "    'document': documents\n",
        "}\n",
        "\n",
        "# For data generation, save to data/raw (not data/processed)\n",
        "# Use absolute path relative to project root (one level up from notebooks)\n",
        "raw_data_path = os.path.abspath('../data/raw') if config['data_source']['type'] == 'csv' else None\n",
        "\n",
        "with Timer(\"Saving Data\"):\n",
        "    for table_name, df in tables_dict.items():\n",
        "        # Pass custom output path for CSV mode\n",
        "        if config['data_source']['type'] == 'csv':\n",
        "            save_data_to_destination(df, config, table_name, spark, output_path=raw_data_path)\n",
        "        else:\n",
        "            save_data_to_destination(df, config, table_name, spark)\n",
        "        print(f\"âœ… Saved {table_name}: {len(df)} records\")\n",
        "\n",
        "print(\"\\nâœ… All data saved successfully!\")\n",
        "print(f\"ðŸ“‚ Files saved to: {raw_data_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Validation Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "                            Data Generation Summary                             \n",
            "================================================================================\n",
            "\n",
            "           Table  Records  Columns\n",
            "         address     1000        6\n",
            " banking_product        7        7\n",
            "         channel        5        3\n",
            "           party     5000        7\n",
            "        customer     5000        4\n",
            "customer_account     8000        9\n",
            "     transaction    50000        5\n",
            "   communication    10000        6\n",
            "        document    15000        6\n",
            "\n",
            "ðŸ“Š Total Records Generated: 94012\n",
            "ðŸ“… Generation Date: 2025-11-12 21:44:16\n",
            "\n",
            "âœ… Data generation notebook completed!\n"
          ]
        }
      ],
      "source": [
        "print_section_header(\"Data Generation Summary\")\n",
        "\n",
        "summary_data = []\n",
        "for table_name, df in tables_dict.items():\n",
        "    summary_data.append({\n",
        "        'Table': table_name,\n",
        "        'Records': len(df),\n",
        "        'Columns': len(df.columns)\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "total_records = sum([item['Records'] for item in summary_data])\n",
        "print(f\"\\nðŸ“Š Total Records Generated: {total_records}\")\n",
        "print(f\"ðŸ“… Generation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"\\nâœ… Data generation notebook completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. Run **01_eda.py** for exploratory data analysis\n",
        "2. Proceed with feature engineering in **02_feature_engineering.py**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hyperpersonalization-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
